{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1a33c67",
   "metadata": {},
   "source": [
    "# Q&A System Correctness\n",
    "\n",
    "====================================\n",
    "\n",
    "### 1. Theory: The Challenge of Evaluating Q&A Systems\n",
    "\n",
    "Evaluating a question-and-answer (Q&A) system, especially one powered by a Large Language Model (LLM), is a complex task. Unlike simple classification or regression models, a Q&A system's output is unstructured, free-form text. A single question can have multiple valid answers, each phrased differently.\n",
    "\n",
    "Traditional text metrics like BLEU or ROUGE, which measure word overlap, often fail to capture the semantic correctness of a lengthy response. An answer could be factually correct but use different words, leading to a low score, or it could be factually wrong while using many of the same keywords as the reference answer. \n",
    "\n",
    "To overcome this, we can leverage **LLM-assisted evaluation**. This approach uses another powerful LLM as an impartial \"judge\" to grade the Q&A system's response based on a reference answer. This allows for a more nuanced understanding of correctness, complementing human review and providing a scalable way to measure performance.\n",
    "\n",
    "In this walkthrough, we will use **LangSmith** to evaluate the correctness of a Retrieval-Augmented Generation (RAG) Q&A system. The process will follow these key steps:\n",
    "\n",
    "1.  **Create a Dataset**: We will build a collection of question-and-answer pairs to serve as our ground truth.\n",
    "2.  **Define the Q&A System**: We will construct a RAG pipeline that retrieves information from documentation and generates an answer.\n",
    "3.  **Run Evaluation**: We will use LangSmith to run our system against the dataset and automatically grade the responses for correctness.\n",
    "4.  **Iterate and Improve**: We will analyze the results to identify failures, modify our system, and re-evaluate to confirm the improvements.\n",
    "\n",
    "The complete test run, including all feedback and traces, will be saved in a LangSmith project for easy analysis.\n",
    "\n",
    "![test project](./img/test_project.png)\n",
    "\n",
    "> **Note 1:** This walkthrough focuses on testing the end-to-end performance of the system. It's also crucial to evaluate individual components. For instance, the retriever can be tested separately using standard information retrieval metrics (e.g., hit rate, MRR) to ensure it's fetching relevant documents effectively.\n",
    "\n",
    "> **Note 2:** If your knowledge base (the documents your system answers from) is constantly changing, your reference answers might become outdated. It's important to have a strategy to manage this, such as freezing the knowledge source during testing or regularly updating your evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-0e1f2a3b4c5d",
   "metadata": {},
   "source": [
    "### 2. Prerequisites and Setup\n",
    "\n",
    "First, we'll configure our environment variables. These are essential for connecting our code to the LangSmith and OpenAI services.\n",
    "\n",
    "- **`LANGCHAIN_ENDPOINT`**: This URL tells LangChain to send all tracing data to the LangSmith platform.\n",
    "- **`LANGCHAIN_API_KEY`**: This is your secret key for authenticating with LangSmith.\n",
    "- **`PROJECT_NAME`**: (Optional) This allows you to group related runs in LangSmith under a specific project. It's highly recommended for organization.\n",
    "\n",
    "This tutorial uses OpenAI models, ChromaDB for the vector store, and LangChain for building the RAG chain. You will also need to set your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c788783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # Import function to load environment variables\n",
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Load environment variables from the .env file. The `override=True` argument\n",
    "# ensures that variables from the .env file will overwrite existing environment variables.\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint as an environment variable.\n",
    "# Update with your API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')# Set your LangSmith API key as an environment variable.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # Set your OpenAI API key as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe780fbd",
   "metadata": {},
   "source": [
    "Next, we install the required Python packages. We use `%pip` to install them directly within the notebook.\n",
    "\n",
    "- `langchain[openai]`: Installs the core LangChain library along with the specific integrations for OpenAI models.\n",
    "- `chromadb`: The vector database we will use to store and retrieve document embeddings.\n",
    "- `lxml`: A robust parser for HTML and XML, used by our document loader.\n",
    "- `html2text`: A utility to convert HTML into clean, readable plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f9e7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '%pip install' command installs python packages. The '> /dev/null' part suppresses the output for a cleaner notebook.\n",
    "# %pip install -U \"langchain[openai]\" > /dev/null\n",
    "# %pip install chromadb > /dev/null\n",
    "# %pip install lxml > /dev/null\n",
    "# %pip install html2text > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3c4d5-e6f7-4a8b-9c0d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "Set your OpenAI API Key. This is required to use OpenAI's embedding and language models. Replace `<YOUR-API-KEY>` with your actual key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afac8079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '%env' magic command sets an environment variable for the notebook session.\n",
    "# %env OPENAI_API_KEY=<YOUR-API-KEY>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80ab6e",
   "metadata": {},
   "source": [
    "## Step 1: Create a Dataset\n",
    "\n",
    "A high-quality dataset is the foundation of any reliable evaluation. For our Q&A system, the dataset will consist of question-answer pairs. The questions represent typical user queries, and the answers are the \"ground truth\" or reference responses we expect the system to provide.\n",
    "\n",
    "For this example, we'll create a dataset about LangSmith documentation. We have hard-coded a few examples below. For a real-world scenario, it's best to have a much larger dataset (e.g., >100 examples) to get statistically significant results. These examples should ideally be sourced from real user interactions to ensure they are representative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e98a80-bf37-457e-b31d-952292e76c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a list of tuples, where each tuple contains a (question, answer) pair.\n",
    "examples = [\n",
    "    (\n",
    "        \"What is LangChain?\", # This is the input question.\n",
    "        \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\", # This is the reference answer.\n",
    "    ),\n",
    "    (\n",
    "        \"How might I query for all runs in a project?\",\n",
    "        \"client.list_runs(project_name='my-project-name'), or in TypeScript, client.ListRuns({projectName: 'my-project-anme'})\",\n",
    "    ),\n",
    "    (\n",
    "        \"What's a langsmith dataset?\",\n",
    "        \"A LangSmith dataset is a collection of examples. Each example contains inputs and optional expected outputs or references for that data point.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How do I use a traceable decorator?\",\n",
    "        \"\"\"The traceable decorator is available in the langsmith python SDK. To use, configure your environment with your API key,\\\n",
    "import the required function, decorate your function, and then call the function. Below is an example:\n",
    "```python\n",
    "from langsmith.run_helpers import traceable\n",
    "@traceable(run_type=\"chain\") # or \"llm\", etc.\n",
    "def my_function(input_param):\n",
    "    # Function logic goes here\n",
    "    return output\n",
    "result = my_function(input_param)\n",
    "```\"\"\",\n",
    "    ),\n",
    "    (\n",
    "        \"Can I trace my Llama V2 llm?\",\n",
    "        \"So long as you are using one of LangChain's LLM implementations, all your calls can be traced\",\n",
    "    ),\n",
    "    (\n",
    "        \"Why do I have to set environment variables?\",\n",
    "        \"Environment variables can tell your LangChain application to perform tracing and contain the information necessary to authenticate to LangSmith.\"\n",
    "        \" While there are other ways to connect, environment variables tend to be the simplest way to configure your application.\",\n",
    "    ),\n",
    "    (\n",
    "        \"How do I move my project between organizations?\",\n",
    "        \"LangSmith doesn't directly support moving projects between organizations.\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b4c5d6-e7f8-4a9b-ac0d-2e3f4a5b6c7d",
   "metadata": {},
   "source": [
    "Now, let's create a LangSmith client, which is our main entry point for interacting with the LangSmith platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5edb7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client # Import the Client class from the langsmith library.\n",
    "\n",
    "client = Client() # Instantiate the client. It will automatically use the environment variables we set earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7-e8f9-4a0b-ad1d-3e4f5a6b7c8d",
   "metadata": {},
   "source": [
    "Using the client, we will programmatically create a new dataset in LangSmith and populate it with our examples. We add a unique identifier (`uuid`) to the dataset name to prevent naming conflicts if we run this notebook multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbcd3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid # Import the uuid library to generate unique identifiers.\n",
    "\n",
    "# Define a unique name for the dataset using a UUID to avoid collisions.\n",
    "dataset_name = f\"Retrieval QA Questions {str(uuid.uuid4())}\"\n",
    "# Create the dataset on the LangSmith platform and get back a dataset object.\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "# Loop through our list of hard-coded question-answer pairs.\n",
    "for q, a in examples:\n",
    "    # For each pair, create an example in our LangSmith dataset.\n",
    "    client.create_example(\n",
    "        inputs={\"question\": q}, # The input dictionary must have keys that match what our chain expects.\n",
    "        outputs={\"answer\": a}, # The output dictionary contains the ground truth reference answer.\n",
    "        dataset_id=dataset.id # We specify which dataset to add this example to.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f9eb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('d07a59ea-1e4b-43fb-a680-adc79ef5f87d')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f622857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retrieval QA Questions 6b1fd88c-9cce-4611-8729-3e6f28046f6e'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976437f",
   "metadata": {},
   "source": [
    "## Step 2. Define the RAG Q&A System\n",
    "\n",
    "Now we'll build our Q&A system. We are using a **Retrieval-Augmented Generation (RAG)** architecture. This is a powerful pattern for building knowledgeable LLM systems. A RAG system works in two main stages:\n",
    "\n",
    "1.  **Retrieval**: Given a user's question, the system first retrieves relevant information from a knowledge base. In our case, this knowledge base is the LangSmith documentation. This stage consists of:\n",
    "    -   An **Embedding Model** (`OpenAIEmbeddings`): Converts both the documents and the user's question into numerical vectors (embeddings).\n",
    "    -   A **Vector Store** (`Chroma`): A specialized database that stores the document vectors and allows for efficient searching to find vectors (and thus documents) that are most similar to the question vector.\n",
    "    -   A **Retriever**: The component that orchestrates the search in the vector store and returns the most relevant documents.\n",
    "\n",
    "2.  **Generation**: The retrieved documents are then passed to an LLM, along with the original question, to generate a final, synthesized answer. This stage consists of:\n",
    "    -   A **Prompt Template** (`ChatPromptTemplate`): Structures the input for the LLM, combining the retrieved context and the user's question with instructions on how to answer.\n",
    "    -   An **LLM** (`ChatOpenAI`): The language model that reads the prompt and generates the textual response.\n",
    "\n",
    "We will use LangChain Expression Language (LCEL) to elegantly combine these components into a single, executable chain.\n",
    "\n",
    "First, let's load and process the documents to populate our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95fab721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import RecursiveUrlLoader # A loader for recursively scraping a website.\n",
    "from langchain_community.document_transformers import Html2TextTransformer # A transformer to convert HTML content to plain text.\n",
    "from langchain_community.vectorstores import Chroma # The Chroma vector store implementation.\n",
    "from langchain_text_splitters import TokenTextSplitter # A text splitter that splits based on token count.\n",
    "from langchain_openai import OpenAIEmbeddings # The class for using OpenAI's embedding models.\n",
    "\n",
    "# Initialize a loader to fetch all documents from the LangSmith documentation website.\n",
    "api_loader = RecursiveUrlLoader(\"https://docs.smith.langchain.com\")\n",
    "# Initialize a text splitter to break large documents into smaller chunks.\n",
    "text_splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-3.5-turbo\", # The model used to count tokens for splitting.\n",
    "    chunk_size=2000, # The maximum size of each chunk in tokens.\n",
    "    chunk_overlap=200, # The number of tokens to overlap between consecutive chunks.\n",
    ")\n",
    "# Initialize a transformer to clean up the raw HTML.\n",
    "doc_transformer = Html2TextTransformer()\n",
    "# Load the raw documents from the URL.\n",
    "raw_documents = api_loader.load()\n",
    "# Transform the raw HTML documents into plain text.\n",
    "transformed = doc_transformer.transform_documents(raw_documents)\n",
    "# Split the transformed documents into smaller, manageable chunks.\n",
    "documents = text_splitter.split_documents(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc4b79-4219-4446-a00c-beda55c2205a",
   "metadata": {},
   "source": [
    "With the documents processed, we can now create the vector store and the retriever. The vector store will embed and index our document chunks, and the retriever will provide the interface for searching them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "057f0841-dd9f-4f75-8ff5-dbdda73f84ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI embeddings model.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "# Create a Chroma vector store from the documents, using the OpenAI embeddings model.\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "# Create a retriever from the vector store, configured to return the top 4 most relevant documents.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e35f884-bb93-427d-a0ad-3858c449a1ea",
   "metadata": {},
   "source": [
    "Next, we define the response generation part of our RAG chain. This involves creating a prompt template that will be populated with the retrieved context and the user's question, and then an LLM to generate the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f266e9-e4de-42ad-b41e-99ace4dc5131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime # Import datetime to include the current time in the prompt.\n",
    "from langchain_core.output_parsers import StrOutputParser # Import the string output parser.\n",
    "from langchain_core.prompts import ChatPromptTemplate # Import the chat prompt template class.\n",
    "from langchain_openai import ChatOpenAI # Import the OpenAI chat model class.\n",
    "\n",
    "# Define the prompt template. This structures the input for the LLM.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\", # The system message provides high-level instructions to the model.\n",
    "            \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "            \" questions from LangSmith's documentation.\"\n",
    "            \" LangChain is a framework for building applications using large language models.\"\n",
    "            \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"), # A placeholder for the retrieved documents (context).\n",
    "        (\"human\", \"{question}\"), # A placeholder for the user's question.\n",
    "    ]\n",
    ").partial(time=str(datetime.now())) # Pre-fill the 'time' variable with the current time.\n",
    "\n",
    "# Initialize the LLM we'll use for generation. We use gpt-3.5-turbo with a large context window and low temperature for factual answers.\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "# Define the response generator part of the chain using LCEL. It pipes the prompt to the model, then to an output parser.\n",
    "response_generator = prompt | model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e63610-fdab-4d4e-80db-cdf38805040d",
   "metadata": {},
   "source": [
    "Finally, we assemble the full RAG chain using LangChain Expression Language (LCEL). This chain will seamlessly connect the retrieval and generation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4490e622-d865-44ee-b6f4-681b658dad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The full chain combines the retriever and the response generator.\n",
    "from operator import itemgetter # Import itemgetter for convenient data routing.\n",
    "\n",
    "chain = (\n",
    "    # A Runnable Map takes the input dictionary and prepares a new dictionary for the next step.\n",
    "    {\n",
    "        # The 'context' key is populated by a sub-chain: get the question, pass it to the retriever, and format the resulting documents.\n",
    "        \"context\": itemgetter(\"question\")\n",
    "        | retriever\n",
    "        | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "        # The 'question' key is passed through directly from the input.\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | response_generator # The output of the map is piped into our response generator chain.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4-f5a6-b7c8-d9e0-f1a2b3c4d5e6",
   "metadata": {},
   "source": [
    "Let's do a quick test of our chain with a single question to see it in action before we run the full evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77b0954d-924b-4241-9c59-96adbd1c3ac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To log user feedback to a run in LangSmith, you can follow these steps:\n",
      "\n",
      "1. **Create a RunTree Object**: First, you need to create a `RunTree` object to represent the run you want to log user feedback for. This object should include the necessary information such as the name of the component, the type of run, inputs, outputs, and any errors.\n",
      "\n",
      "2. **Post the Run**: After creating the `RunTree` object, you need to post the run to LangSmith to initiate the logging process.\n",
      "\n",
      "3. **End the Run with User Feedback**: Once the run is completed and you have collected user feedback, you can end the run by providing the user feedback as part of the outputs. This step allows you to log the user feedback along with the run data.\n",
      "\n",
      "4. **Patch the Run**: Finally, you can patch the run to update any additional information or finalize the logging process.\n",
      "\n",
      "Here is a code snippet demonstrating how you can log user feedback to a run using the LangSmith SDK:\n",
      "\n",
      "```typescript\n",
      "import { RunTree } from \"langsmith\";\n",
      "\n",
      "// Create a RunTree object\n",
      "const runConfig = {\n",
      "  name: \"My Component\",\n",
      "  run_type: \"llm\",\n",
      "  inputs: {\n",
      "    text: \"User input text here\",\n",
      "  },\n",
      "  // Add any other necessary attributes\n",
      "};\n",
      "\n",
      "const run = new RunTree(runConfig);\n",
      "\n",
      "// Post the run\n",
      "await run.postRun();\n",
      "\n",
      "// End the run with user feedback\n",
      "await run.end({\n",
      "  outputs: {\n",
      "    userFeedback: \"User feedback goes here\",\n",
      "  },\n",
      "});\n",
      "\n",
      "// Patch the run\n",
      "await run.patchRun();\n",
      "```\n",
      "\n",
      "By following these steps and using the LangSmith SDK, you can effectively log user feedback to a run in your LangSmith project."
     ]
    }
   ],
   "source": [
    "# We stream the output of the chain for a sample question.\n",
    "for tok in chain.stream({\"question\": \"How do I log user feedback to a run?\"}):\n",
    "    print(tok, end=\"\", flush=True) # Print each token as it is generated for a real-time effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dddd61",
   "metadata": {},
   "source": [
    "## Step 3. Evaluate the Chain\n",
    "\n",
    "With our chain defined and our dataset ready, it's time to run the evaluation. We will use one of LangSmith's built-in, LLM-assisted evaluators called `\"qa\"`. This evaluator is specifically designed for Q&A tasks. For each example in our dataset, it will:\n",
    "1.  Receive the generated answer from our RAG chain.\n",
    "2.  Receive the reference answer from our dataset.\n",
    "3.  Use an LLM to determine if the generated answer is a \"correct\" answer based on the reference. It returns a binary score (1 for correct, 0 for incorrect).\n",
    "\n",
    "We configure this using the `RunEvalConfig` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dedaff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig # Import the evaluation configuration class.\n",
    "\n",
    "# Create an evaluation configuration object.\n",
    "eval_config = RunEvalConfig(\n",
    "    # Specify the evaluators to use. 'qa' is a built-in evaluator for question-answering correctness.\n",
    "    evaluators=[\"qa\"],\n",
    "    # You can optionally configure the LLM used for evaluation if you want to use a different model.\n",
    "    # eval_llm=ChatAnthropic(model=\"claude-2\", temperature=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5e4f7-f5b4-4d6a-9dd7-1973d8e9c0f7",
   "metadata": {},
   "source": [
    "Now we execute the evaluation. The `client.arun_on_dataset` function orchestrates the entire process. It iterates through each example in our dataset, runs our RAG chain on the input question, and then applies the `qa` evaluator to score the result. The `await` keyword is used because this is an asynchronous operation, which can run evaluations in parallel for greater efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f30ce874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'extraneous-sea-89' at:\n",
      "https://smith.langchain.com/o/0212d326-bd9d-42bb-9937-c063f40f2361/datasets/d07a59ea-1e4b-43fb-a680-adc79ef5f87d/compare?selectedSessions=d64011af-44ac-4b1d-afd5-debc5d7e7035\n",
      "\n",
      "View all tests for Dataset Retrieval QA Questions 6b1fd88c-9cce-4611-8729-3e6f28046f6e at:\n",
      "https://smith.langchain.com/o/0212d326-bd9d-42bb-9937-c063f40f2361/datasets/d07a59ea-1e4b-43fb-a680-adc79ef5f87d\n",
      "[------------------------------------------------->] 7/7\n"
     ]
    }
   ],
   "source": [
    "# Asynchronously run the evaluation on the dataset.\n",
    "_ = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name, # The name of the dataset to test against.\n",
    "    llm_or_chain_factory=lambda: chain, # A function that returns an instance of the chain to be tested.\n",
    "    evaluation=eval_config, # The evaluation configuration we defined earlier.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fac17c-e0c7-4d85-bca1-18c4337fbcba",
   "metadata": {},
   "source": [
    "### Analyzing the Results in LangSmith\n",
    "\n",
    "As the test progresses, you can click the link printed above to go to the LangSmith project. There, you can see real-time results, including the chain's outputs, the feedback scores from the evaluator, and detailed traces for each run.\n",
    "\n",
    "To find problematic examples, you can filter the results. For example, to see all the runs that the `qa` evaluator marked as incorrect, you can filter for `\"Correctness==0\"`.\n",
    "\n",
    "![Incorrect Examples](./img/filter_correctness.png)\n",
    "\n",
    "Clicking on an individual run lets you inspect the full trace to understand what went wrong. The \"Feedback\" tab within the trace view shows the reasoning behind the evaluator's score. \n",
    "\n",
    "![Incorrect Example Trace](./img/see_trace.png)\n",
    "\n",
    "Since LLM-assisted evaluations are themselves LLM runs, you can even inspect the trace of the evaluator itself. This is useful for auditing the evaluation process and ensuring the evaluator is behaving as expected. You can click the link highlighted in the image to see the evaluator's thought process.\n",
    "\n",
    "![QA Eval Chain Run](./img/qa_eval_chain_run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15270951-84df-4ac8-84ef-23312fc16db0",
   "metadata": {},
   "source": [
    "### Diagnosing and Fixing the Error\n",
    "In this example, one of the traces was marked as \"incorrect\". By inspecting the trace, we might find that the model is \"hallucinating\" – making up information that wasn't present in the retrieved documents. \n",
    "\n",
    "LangSmith's **Playground** is an interactive environment for debugging and improving prompts. By clicking on a specific LLM call in a trace, you can open it in the Playground to experiment with changes.\n",
    "\n",
    "![Open in Playground](./img/open_in_playground.png)\n",
    "\n",
    "To fix the hallucination, we can try making the prompt more robust. Let's add an explicit instruction telling the model to *only* use the provided documents and to admit when it doesn't know the answer. We can add a new system message:\n",
    "\n",
    "> Respond as best as you can. If no documents are retrieved or if you do not see an answer in the retrieved documents, admit you do not know or that you don't see it being supported at the moment.\n",
    "\n",
    "After adding this message in the Playground and resubmitting, we can see if the model's behavior improves.\n",
    "\n",
    "![Change Prompt](./img/playground_prompt.png)\n",
    "\n",
    "The new prompt seems to fix the issue for this specific example. However, we need to ensure this change doesn't negatively affect other examples (i.e., we're not overfitting to a single failure case). The next step is to re-run the entire evaluation with our improved chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbd6a6-e3eb-4d3a-afe6-d5917d81c54d",
   "metadata": {},
   "source": [
    "## Step 4. Iterate and Re-Evaluate\n",
    "\n",
    "Evaluation is not a one-time event; it's a cycle. We analyzed our initial results, identified a problem, and prototyped a fix in the Playground. Now, we'll implement that fix in our code and re-run the evaluation to measure the impact of our change across the entire dataset.\n",
    "\n",
    "Below, we define a new RAG chain (`chain_2`) that includes the improved prompt with the added system message to discourage hallucination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25d189e9-ff07-48d9-9aef-7fc17b265e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new, improved prompt template.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful documentation Q&A assistant, trained to answer\"\n",
    "            \" questions from LangSmith's documentation.\"\n",
    "            \"\\nThe current time is {time}.\\n\\nRelevant documents will be retrieved in the following messages.\",\n",
    "        ),\n",
    "        (\"system\", \"{context}\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "        # Add the new system message here to make the model more cautious:\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Respond as best as you can. If no documents are retrieved or if you do not see an answer in the retrieved documents,\"\n",
    "            \" admit you do not know or that you don't see it being supported at the moment.\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(time=lambda: str(datetime.now())) # Use a lambda to get the current time dynamically for each run.\n",
    "\n",
    "# Re-initialize the model and response generator with the new prompt.\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0)\n",
    "response_generator_2 = prompt | model | StrOutputParser()\n",
    "# Assemble the second version of our RAG chain.\n",
    "chain_2 = {\n",
    "    \"context\": itemgetter(\"question\")\n",
    "    | retriever\n",
    "    | (lambda docs: \"\\n\".join([doc.page_content for doc in docs])),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "} | response_generator_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf882f9-c75b-43ac-956f-acd8f17ef800",
   "metadata": {},
   "source": [
    "Now we run the evaluation again, this time pointing to `chain_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbdfa2f4-fa96-42d9-94bc-bb5c227104da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'crushing-chain-51' at:\n",
      "https://smith.langchain.com/o/0212d326-bd9d-42bb-9937-c063f40f2361/datasets/d07a59ea-1e4b-43fb-a680-adc79ef5f87d/compare?selectedSessions=d053012a-72c8-4d7d-9d45-e8b429683467\n",
      "\n",
      "View all tests for Dataset Retrieval QA Questions 6b1fd88c-9cce-4611-8729-3e6f28046f6e at:\n",
      "https://smith.langchain.com/o/0212d326-bd9d-42bb-9937-c063f40f2361/datasets/d07a59ea-1e4b-43fb-a680-adc79ef5f87d\n",
      "[------------------------------------------------->] 7/7\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation again with the updated chain factory.\n",
    "_ = await client.arun_on_dataset(\n",
    "    dataset_name=dataset_name, # Use the same dataset as before.\n",
    "    llm_or_chain_factory=lambda: chain_2, # Point to the new, improved chain.\n",
    "    evaluation=eval_config, # Use the same evaluation configuration.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5352e20-af87-4735-9061-caf09e86610c",
   "metadata": {},
   "source": [
    "### Comparing Results\n",
    "\n",
    "After the second run is complete, we can go to our dataset page in LangSmith to compare the performance of the two chains. LangSmith automatically aggregates the feedback scores for each test run associated with a dataset.\n",
    "\n",
    "![Datasets Page](./img/dataset_test_runs.png)\n",
    "\n",
    "In this case, it looks like our change was successful, and the new chain now passes all the examples! This is a great result. Remember that this is a small, illustrative dataset; with a larger dataset, the goal is to see a positive trend in the aggregate correctness score.\n",
    "\n",
    "LangSmith also makes it easy to compare outputs at the individual example level. In the \"Examples\" tab of the dataset, you can click on any row to see a side-by-side comparison of the responses from both test runs, making it easy to spot qualitative differences in their behavior.\n",
    "\n",
    "![Example Page](./img/example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e3e141-30aa-4427-8b79-73f9535872ca",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "Congratulations! You have successfully completed a full evaluation cycle for a RAG Q&A system. \n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "- Create a labeled dataset for evaluation in LangSmith.\n",
    "- Build a complete RAG pipeline using LangChain.\n",
    "- Use an LLM-assisted evaluator (`qa`) to automatically measure the correctness of your system's answers.\n",
    "- Use the insights from LangSmith to diagnose a failure (hallucination), iterate on your prompt, and verify the improvement by re-running the evaluation.\n",
    "\n",
    "This iterative process of testing, analyzing, and improving is fundamental to building reliable and high-quality LLM applications. \n",
    "\n",
    "Thanks for trying this out! If you have questions or suggestions, please open an issue on GitHub or reach out to us at support@langchain.dev."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
