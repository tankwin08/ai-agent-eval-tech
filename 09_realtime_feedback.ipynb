{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bddb3342-ae12-4d6b-98d1-21cb0674c1d0",
   "metadata": {},
   "source": [
    "# Real-time Automated Feedback\n",
    "\n",
    "====================================\n",
    "\n",
    "### 1. Theory: From Offline Evaluation to Real-time Monitoring\n",
    "\n",
    "While evaluating your LLM applications on a static dataset is crucial for development, monitoring their performance in a live production environment presents a different challenge. You want to understand how your application is performing on real-world, unpredictable inputs, often without having ground-truth labels.\n",
    "\n",
    "This is where **real-time automated feedback** comes in. The core idea is to use another LLM as an impartial judge to score your application's outputs as they happen. This is achieved using **reference-free evaluators**â€”metrics like \"helpfulness,\" \"conciseness,\" or \"lack of toxicity\" that do not require a predefined \"correct\" answer. \n",
    "\n",
    "This tutorial demonstrates how to attach such an evaluator as a **callback** to your chain. This causes the evaluator to run automatically and asynchronously every time your chain is invoked, sending the resulting feedback scores to LangSmith. LangSmith can then aggregate this feedback, allowing you to create monitoring charts and track the quality of your deployment over time.\n",
    "\n",
    "![model-based feedback monitoring charts](./img/feedback_charts.png)\n",
    "\n",
    "If these automated metrics reveal a drop in quality, you can easily filter and isolate the problematic runs in LangSmith for debugging, analysis, or for creating new evaluation datasets.\n",
    "\n",
    "The process involves two main steps:\n",
    "\n",
    "1.  **Define Feedback Logic**: We'll create a custom `RunEvaluator` that encapsulates the logic for scoring a run (in this case, for \"helpfulness\").\n",
    "2.  **Include in Callbacks**: We'll use the `EvaluatorCallbackHandler` to attach our evaluator to the chain, ensuring it runs automatically after each invocation.\n",
    "\n",
    "We'll be using LangSmith, so make sure you have the necessary API keys configured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "### 2. Prerequisites and Setup\n",
    "\n",
    "First, we'll install the necessary Python packages and configure our environment variables to connect to LangSmith and the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba38d55-6551-49c2-a3cb-72a56e3c3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '%pip install' command installs python packages from the notebook.\n",
    "# -U flag ensures we get the latest versions.\n",
    "# --quiet suppresses the installation output for a cleaner interface.\n",
    "# %pip install -U langchain openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f496bc2-d9c6-46ea-be19-98ff2dd2800c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Set the environment variable to enable LangSmith tracing.\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# # Update with your API URL if using a hosted instance of Langsmith.\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# # Update with your API key.\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"\n",
    "# Change to your project name to organize runs in LangSmith.\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"realtime_monitoring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ab43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # Import function to load environment variables\n",
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Load environment variables from the .env file. The `override=True` argument\n",
    "# ensures that variables from the .env file will overwrite existing environment variables.\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint as an environment variable.\n",
    "# Update with your API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')# Set your LangSmith API key as an environment variable.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # Set your OpenAI API key as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec884ee8-c4af-4bc5-b9e7-e87fa4c4f97f",
   "metadata": {},
   "source": [
    "Once you've decided on the runs you want to evaluate, it's time to define the feedback pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efb84a-c84a-4903-95a8-e6c0d1e47fa4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Define Feedback Logic\n",
    "\n",
    "The first step is to define the logic that will generate our feedback scores. In LangSmith, all feedback must have a `key` (like \"helpfulness\") and a nullable numeric `score`. \n",
    "\n",
    "We will create a custom `RunEvaluator` class. This class will wrap one of LangChain's built-in `score_string` evaluators. This off-the-shelf evaluator uses an LLM to judge an input/prediction pair against a specified criterion. Our custom class will handle the logic of extracting the correct input and output from the run trace and passing them to the underlying scoring evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7a8e56-ad86-4e7e-809b-f86704f0cd2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional # Import typing hints.\n",
    "from langchain.evaluation import load_evaluator # Import a helper to load built-in evaluators.\n",
    "from langsmith.evaluation import RunEvaluator, EvaluationResult # Import the base classes for custom evaluation.\n",
    "from langsmith.schemas import Run, Example # Import the Run and Example schemas from LangSmith.\n",
    "\n",
    "\n",
    "# Define our custom evaluator class, inheriting from the base RunEvaluator.\n",
    "class HelpfulnessEvaluator(RunEvaluator):\n",
    "    def __init__(self):\n",
    "        # Initialize a built-in 'score_string' evaluator for the 'helpfulness' criterion.\n",
    "        self.evaluator = load_evaluator(\n",
    "            \"score_string\", criteria=\"helpfulness\", normalize_by=10\n",
    "        )\n",
    "\n",
    "    # This is the core method that will be called for each run.\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> EvaluationResult:\n",
    "        # A safety check to ensure the run has the necessary inputs and outputs.\n",
    "        if (\n",
    "            not run.inputs\n",
    "            or not run.inputs.get(\"input\")\n",
    "            or not run.outputs\n",
    "            or not run.outputs.get(\"output\")\n",
    "        ):\n",
    "            # Return a null score if the required fields are not present.\n",
    "            return EvaluationResult(key=\"helpfulness\", score=None)\n",
    "        # Call the underlying evaluator with the run's input and output.\n",
    "        result = self.evaluator.evaluate_strings(\n",
    "            input=run.inputs[\"input\"], prediction=run.outputs[\"output\"]\n",
    "        )\n",
    "        # Return the final result, mapping the keys to the LangSmith feedback format.\n",
    "        return EvaluationResult(\n",
    "            **{\"key\": \"helpfulness\", \"comment\": result.get(\"reasoning\"), **result}\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd061c7-039c-4799-8eeb-2acffc082545",
   "metadata": {},
   "source": [
    "By defining this `RunEvaluator`, we have created a reusable component that can generate \"helpfulness\" feedback for any run, linking the feedback to the specific evaluation trace that produced it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c73207-5ba2-40e9-85c3-c9b0401b5702",
   "metadata": {},
   "source": [
    "## Step 2: Include the Evaluator in Callbacks\n",
    "\n",
    "Now we'll use the **`EvaluatorCallbackHandler`**. This is a special LangChain callback that takes our custom evaluator and runs it automatically in a separate thread whenever a run is completed. This is the key to achieving real-time feedback without blocking our main application.\n",
    "\n",
    "First, let's define the simple chain we want to monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a465ce-7e59-411b-b53a-f60d23bfaa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser # Import the string output parser.\n",
    "from langchain_core.prompts import ChatPromptTemplate # Import the chat prompt template.\n",
    "from langchain_openai import ChatOpenAI # Import the OpenAI chat model wrapper.\n",
    "\n",
    "# Define a simple chain: it takes an input, passes it to a chat model, and parses the output as a string.\n",
    "chain = (\n",
    "    ChatPromptTemplate.from_messages([(\"user\", \"{input}\")])\n",
    "    | ChatOpenAI(model = \"gpt-3.5-turbo\" )# Specify the model to use.\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907cd516-b175-46fe-b056-f8967be37930",
   "metadata": {},
   "source": [
    "Next, we create an instance of our evaluator and pass it to the `EvaluatorCallbackHandler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801fc59a-0575-49c9-be5a-81ce6ae0b39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This chain was only tested with GPT-4. Performance may be significantly worse with other models.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tracers import EvaluatorCallbackHandler # Import the callback handler.\n",
    "\n",
    "# Create an instance of our custom evaluator.\n",
    "evaluator = HelpfulnessEvaluator()\n",
    "\n",
    "# Create the callback handler, passing a list containing our evaluator instance.\n",
    "feedback_callback = EvaluatorCallbackHandler(evaluators=[evaluator])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4-b5c6-d7e8-f9a0-d2e3f4a5b6c7",
   "metadata": {},
   "source": [
    "Finally, we invoke our chain for a series of queries. The crucial step is to pass our `feedback_callback` in the `config` dictionary of the `invoke` method. This attaches the handler to the run, ensuring our evaluator is triggered automatically upon completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37390889-b2f6-43db-a8c6-1d600b3ef24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of example queries to run through our chain.\n",
    "queries = [\n",
    "    \"Where is Antioch?\",\n",
    "    \"What was the US's inflation rate in 2018?\",\n",
    "    \"Who were the stars in the show Friends?\",\n",
    "    \"How much wood could a woodchuck chuck if a woodchuck could chuck wood?\",\n",
    "    \"Why is the sky blue?\",\n",
    "    \"When is Rosh hashanah in 2023?\",\n",
    "]\n",
    "\n",
    "# Loop through each query.\n",
    "for query in queries:\n",
    "    # Invoke the chain, passing the query and the callback handler in the config.\n",
    "    chain.invoke({\"input\": query}, {\"callbacks\": [feedback_callback]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66e144-141a-4d23-b7c3-3627d8ae6265",
   "metadata": {},
   "source": [
    "Now, check your project in LangSmith. You will see the runs appear, and shortly after, the \"helpfulness\" feedback scores will be automatically attached to each run.\n",
    "\n",
    "![feedback_result](./img/feedback_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb41ef-3892-4385-8830-c6decfbf8f5c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have successfully configured a reference-free evaluator to run automatically every time your chain is called. This is a powerful technique for gathering real-time performance metrics on your live LLM applications.\n",
    "\n",
    "By logging this automated feedback to LangSmith, you can create robust monitoring dashboards, track quality over time, and quickly identify and debug issues as they arise in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents-eval-techniques (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
