{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e3b17b-736a-4991-8122-11bf3ac121c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system's environment variables.\n",
    "import uuid # Import the 'uuid' module, though it is not used in this specific cell, it's good practice for creating unique IDs.\n",
    "\n",
    "uid = uuid.uuid4() # Create a unique identifier object (not used here but declared).\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\" # Set your LangSmith API key as an environment variable.\n",
    "# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-ant-***\" # Set your Anthropic (Claude) API key as an environment variable.\n",
    "# # Update with your API URL if using a hosted instance of Langsmith.\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542e26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # Import function to load environment variables\n",
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Load environment variables from the .env file. The `override=True` argument\n",
    "# ensures that variables from the .env file will overwrite existing environment variables.\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint as an environment variable.\n",
    "# Update with your API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')# Set your LangSmith API key as an environment variable.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # Set your OpenAI API key as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876b92c-74a5-47ae-9efc-dab64bf88d19",
   "metadata": {},
   "source": [
    "### Step 1: Create the Evaluation Dataset\n",
    "\n",
    "A robust evaluation starts with a high-quality dataset. For this task, we need examples that pair unstructured text (a contract) with its corresponding structured representation (the filled-out contract details).\n",
    "\n",
    "We will use a pre-existing public dataset in LangSmith, which is derived from the Contract Understanding Atticus Dataset (CUAD). LangSmith's ability to share and clone datasets is incredibly useful for collaboration and reproducibility. The `clone_public_dataset` function will copy this public dataset into your own LangSmith account, allowing you to run evaluations against it.\n",
    "\n",
    "You can explore the original public dataset here: [Contract Extraction Dataset](https://smith.langchain.com/public/08ab7912-006e-4c00-a973-0f833e74907b/d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "921efddb-8210-4d5f-8705-41e6f9521b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client # Import the Client class to interact with the LangSmith API.\n",
    "\n",
    "# The URL of the public dataset we want to use.\n",
    "dataset_url = (\n",
    "    \"https://smith.langchain.com/public/08ab7912-006e-4c00-a973-0f833e74907b/d\"\n",
    ")\n",
    "# Define a name for our local copy of the dataset.\n",
    "dataset_name = f\"Contract Extraction\"\n",
    "\n",
    "# Instantiate the LangSmith client.\n",
    "client = Client()\n",
    "# Use the client to clone the public dataset into your LangSmith account.\n",
    "client.clone_public_dataset(dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a9e51-7fcb-4de5-87af-644b7ca9b893",
   "metadata": {},
   "source": [
    "### Step 2: Define the Extraction Chain\n",
    "\n",
    "Now we'll build the chain responsible for the extraction. The input documents in our dataset are quite long, making this a challenging task. We'll leverage the experimental **Anthropic Functions** capability in LangChain. This feature uses an Anthropic model's underlying ability to follow instructions and generate structured output (in this case, XML) that conforms to a schema we provide.\n",
    "\n",
    "The first step in defining our chain is to create the target schema using Pydantic. This tells the LLM exactly what information to look for and how to structure it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e51793dc-ee9f-491c-aa91-fb32cf66308c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union # Import typing hints for defining our data models.\n",
    "\n",
    "from pydantic import BaseModel # Import BaseModel from Pydantic to create our data schemas.\n",
    "\n",
    "\n",
    "# Define the schema for a physical address.\n",
    "class Address(BaseModel):\n",
    "    street: str # The street address.\n",
    "    city: str # The city.\n",
    "    state: str # The state or province.\n",
    "    zip_code: str # The postal or zip code.\n",
    "    country: Optional[str] # The country, marked as optional.\n",
    "\n",
    "\n",
    "# Define the schema for a party involved in the contract.\n",
    "class Party(BaseModel):\n",
    "    name: str # The name of the party.\n",
    "    address: Address # A nested Address object.\n",
    "    type: Optional[str] # The type of party (e.g., \"Landlord\", \"Tenant\"), marked as optional.\n",
    "\n",
    "\n",
    "# Define the schema for a single section of the contract.\n",
    "class Section(BaseModel):\n",
    "    title: str # The title of the section.\n",
    "    content: str # The full text content of the section.\n",
    "\n",
    "\n",
    "# Define the top-level schema for the entire contract.\n",
    "class Contract(BaseModel):\n",
    "    document_title: str # The main title of the contract document.\n",
    "    exhibit_number: Optional[str] # Any exhibit number associated with the contract, optional.\n",
    "    effective_date: str # The date the contract becomes effective.\n",
    "    parties: List[Party] # A list of Party objects.\n",
    "    sections: List[Section] # A list of Section objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e90fd2-bd7c-499d-b02d-3eee98550031",
   "metadata": {},
   "source": [
    "With our Pydantic schema defined, we can now construct the full extraction chain. This chain will take the raw text of a contract as input and produce the structured `Contract` object as output. We will use LangChain Expression Language (LCEL) to pipe the components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a479e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub # Import the hub to pull pre-made prompts.\n",
    "\n",
    "contract_prompt = hub.pull(\"wfh/anthropic_contract_extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0261b29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'wfh', 'lc_hub_repo': 'anthropic_contract_extraction', 'lc_hub_commit_hash': 'a35e48d739553550a6ead65afc3d38c51a1d1691dd6eac215068a99f30738d53'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template=\"Populate a Contract based on the raw passage below. Use the the provided 'information_extraction' function.\\n\\nIf a property is not present and is not required in the function parameters, do not include it in the output.\\n\\n<Raw Passage>\\n{input}\\n</Raw Passage>\\n\\nRemember to respond using the XML-formating like so:\\n<tool>information_extraction</tool>\\n<tool_input>...THE TOOL INPUT</tool_input>\\n\\nBegin!\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contract_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b491a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b47332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `create_extraction_chain` not found.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a18eb63-c525-40d6-be1d-bd4c52fbfbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub # Import the hub to pull pre-made prompts.\n",
    "from langchain.chains import create_extraction_chain # Import a helper function to easily create an extraction chain.\n",
    "from langchain_anthropic import ChatAnthropic # Import the Anthropic chat model wrapper.\n",
    "from langchain_experimental.llms.anthropic_functions import AnthropicFunctions # Import the experimental Anthropic Functions wrapper.\n",
    "\n",
    "# Pull a prompt that is specifically designed for contract extraction with Anthropic models.\n",
    "contract_prompt = hub.pull(\"wfh/anthropic_contract_extraction\")\n",
    "\n",
    "\n",
    "# Create the core extraction logic as a sub-chain.\n",
    "extraction_subchain = create_extraction_chain(\n",
    "    Contract.schema(), # Provide the Pydantic schema that we want to extract.\n",
    "    llm=AnthropicFunctions(model=\"claude-2.1\", max_tokens=20_000), # Use the Anthropic Functions LLM, specifying a model and max tokens.\n",
    "    prompt=contract_prompt, # Provide the specialized prompt from the hub.\n",
    ")\n",
    "# The dataset provides input with a key named \"context\", but our extraction_subchain expects a key named \"input\".\n",
    "# We use LCEL to create a final chain that correctly maps the keys.\n",
    "chain = (\n",
    "    # This lambda function takes the original input 'x' and transforms it into the format the subchain expects.\n",
    "    (lambda x: {\"input\": x[\"context\"]})\n",
    "    | extraction_subchain # The transformed input is piped into our extraction subchain.\n",
    "    # The subchain's output is `{'text': [...]}`. We transform it again to `{'output': [...]}` for the evaluator.\n",
    "    | (lambda x: {\"output\": x[\"text\"]})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee760e32-6d0b-421a-b73b-c15c60f35883",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate the Chain\n",
    "\n",
    "Now we will evaluate our chain's performance. For structured data like JSON, a simple string comparison is often too strict. The order of keys in a JSON object can change without altering the meaning of the data. \n",
    "\n",
    "To handle this, we'll use the **`json_edit_distance`** evaluator. This is a powerful, non-LLM-based evaluator that works as follows:\n",
    "1.  **Canonicalization**: It takes both the predicted JSON and the reference JSON and standardizes them. This typically involves sorting all keys alphabetically at every level of the object.\n",
    "2.  **String Conversion**: It converts the canonical JSON objects into strings.\n",
    "3.  **Edit Distance**: It calculates the Damerau-Levenshtein edit distance between the two strings. This measures the number of insertions, deletions, substitutions, and transpositions required to change one string into the other.\n",
    "4.  **Normalization**: The raw distance is normalized to produce a similarity score between 0.0 (completely different) and 1.0 (identical).\n",
    "\n",
    "This method provides a fast, cheap, and deterministic way to measure structural and content similarity between the extracted and reference data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4-a5b6-c7d8-e9f0-a1b2c3d4e5f6",
   "metadata": {},
   "source": [
    "Before running the evaluation, we'll adjust the logging level. The legal documents are very long, and if an error occurs during processing, the default logging behavior might print the entire document to the screen, cluttering the notebook. By setting the logging level to `CRITICAL`, we ensure that only the most severe errors are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65d8ff4f-f5fa-4f21-a412-9a8b21749ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging # Import the logging module to control log message verbosity.\n",
    "\n",
    "# We will suppress any non-critical errors here since the documents are long\n",
    "# and could pollute the notebook output with excessive text.\n",
    "logger = logging.getLogger() # Get the root logger.\n",
    "logger.setLevel(logging.CRITICAL) # Set its level to CRITICAL, so only messages of that severity or higher will be shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e3f4a5-b6c7-d8e9-f0a1-b2c3d4e5f6a7",
   "metadata": {},
   "source": [
    "Finally, we run the evaluation using LangSmith's `evaluate` function. This function orchestrates the entire process: it takes our chain, runs it on each example from our dataset, and then applies the specified evaluator to score the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03383cdd-8fef-480f-bddf-b0616ed6e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gf/6rnp_mbx5914kx7qmmh7xzmw0000gn/T/ipykernel_72212/1408924987.py:3: UserWarning: Function evaluate is in beta.\n",
      "  res = evaluate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'ordinary-hat-82' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/fbc1a41c-7043-4b5f-b6e8-78266faac187/compare?selectedSessions=02fbe581-47ae-4c87-bcad-7a8c44e8789b\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65138131988b4c1fa75e2d21cf74fb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate # Import the necessary evaluation functions from LangSmith.\n",
    "\n",
    "# Call the main evaluate function to run the experiment.\n",
    "res = evaluate(\n",
    "    chain.invoke, # The function to be tested. `chain.invoke` is the standard way to run a chain.\n",
    "    data=dataset_name, # The name of the dataset in LangSmith to run on.\n",
    "    evaluators=[LangChainStringEvaluator(\"json_edit_distance\")], # A list of evaluators to apply to the results.\n",
    "    # To avoid hitting API rate limits, we can limit the number of concurrent requests.\n",
    "    max_concurrency=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1851fdbd-64c1-4399-8b2a-4ea04e16fe56",
   "metadata": {},
   "source": [
    "### Step 4: Analyze the Results\n",
    "\n",
    "Now that you've run the evaluation, it's time to inspect the results. Click the link printed in the output above to navigate to the LangSmith project. There, you can see the detailed results for each example in the dataset.\n",
    "\n",
    "- Look at examples with low scores. What kind of errors is the model making? Is it missing entire sections, or just making small mistakes in fields like dates or addresses?\n",
    "- Are there any examples where the model seems to hallucinate information that wasn't in the original text?\n",
    "- Could the reference data in the dataset be improved? Sometimes, evaluation failures can point to ambiguities or errors in the ground truth.\n",
    "\n",
    "This analysis is the most critical part of the iterative development loop, providing the insights needed to improve the prompt, the model choice, or the dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2daf24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f59fb9d-9cb1-4365-9619-28d2897e0dfd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this walkthrough, we demonstrated a methodical approach to building and evaluating a structured data extraction chain. We defined a clear target schema, constructed a chain using a specialized model and prompt, and then measured its performance using a robust, structure-aware evaluation metric.\n",
    "\n",
    "You can apply these same techniques to evaluate any chain that is intended to produce structured output, providing a quantitative and qualitative foundation for improving your LLM applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
