{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e537e271-4b08-491f-8cf7-c9be1f3fcf15",
   "metadata": {},
   "source": [
    "# Exact Match Evaluation\n",
    "\n",
    "====================================\n",
    "\n",
    "### 1. Theory: Understanding Exact Match Evaluation\n",
    "\n",
    "When evaluating Large Language Models (LLMs) or the systems built on top of them, we need a way to measure their performance. The goal is to determine if the model's output is \"correct.\" The simplest and most stringent method for this is **Exact Match Evaluation**.\n",
    "\n",
    "**What is it?**\n",
    "Exact Match evaluation is a binary scoring method where a model's generated output is compared directly against a predefined, ground-truth answer (a \"reference label\"). The output is considered correct (Score: 1) if and only if it is an identical string to the reference label. If there is any difference whatsoever—even a single character, a punctuation mark, or a whitespace variation—the output is considered incorrect (Score: 0).\n",
    "\n",
    "**When is it useful?**\n",
    "This type of evaluation is most effective for tasks where there is a single, unambiguous, and correct answer. Examples include:\n",
    "- **Fact-based Question Answering:** \"What is the capital of France?\" -> \"Paris\"\n",
    "- **Data Extraction:** Extracting a specific date, number, or name from a text.\n",
    "- **Structured Output Generation:** Generating a specific JSON key or a pre-defined command.\n",
    "\n",
    "In this notebook, we will use **LangSmith**, a platform for LLM development and monitoring, to perform exact match evaluation. We will demonstrate this in two ways:\n",
    "1.  Using LangChain's pre-built `\"exact_match\"` evaluator.\n",
    "2.  Creating our own custom evaluator from scratch to replicate the same logic, which shows the flexibility of the platform.\n",
    "\n",
    "You can preview the final results of this notebook on a public LangSmith run [here](https://smith.langchain.com/public/454c80b5-9809-4f4f-95ee-1f71d8e3ef53/d).\n",
    "\n",
    "[![Test graph](./img/result_example.png)](https://smith.langchain.com/public/454c80b5-9809-4f4f-95ee-1f71d8e3ef53/d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a25d7b-9f0a-4c2d-9b5b-5a1e8a9f3b1c",
   "metadata": {},
   "source": [
    "### 2. Setup: Installing Dependencies\n",
    "\n",
    "This first code cell handles the installation of the necessary Python libraries. \n",
    "- `langchain`: The core library for building applications with LLMs.\n",
    "- `langchain_openai`: Provides specific integrations for using OpenAI's models within the LangChain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "121dcc53-70ec-48df-adac-cbd424c66adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `%pip` command is used to install Python packages directly from a Jupyter cell.\n",
    "# The `-U` flag ensures that the packages are upgraded to their latest versions.\n",
    "# The `--quiet` flag suppresses the installation output for a cleaner notebook.\n",
    "# %pip install -U --quiet langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e8b1a3-c5f1-4a3b-9b8a-1a8c7e9d8f7b",
   "metadata": {},
   "source": [
    "### 3. Configuration: Setting Up Environment Variables\n",
    "\n",
    "To connect our application to external services like LangSmith and OpenAI, we need to provide API keys. Storing these keys as environment variables is a security best practice, as it avoids hardcoding them directly in the script.\n",
    "\n",
    "- **`LANGCHAIN_ENDPOINT`**: This tells LangChain where to send the logging and tracing data. We point it to the LangSmith API endpoint.\n",
    "- **`LANGCHAIN_API_KEY`**: This is your personal key to authenticate with your LangSmith account, allowing you to create datasets and log evaluation runs.\n",
    "- **`OPENAI_API_KEY`**: This is your key for the OpenAI API, which is required to make calls to models like `gpt-3.5-turbo`.\n",
    "\n",
    "**Action Required:** You must replace the placeholder values (`\"YOUR API KEY\"` and `\"Your openai api key\"`) with your actual keys for this notebook to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ce718d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv # Import function to load environment variables\n",
    "\n",
    "# Load environment variables from the .env file. The `override=True` argument\n",
    "# ensures that variables from the .env file will overwrite existing environment variables.\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f610c6e-144b-47c8-9791-eaf4f42a8ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint as an environment variable.\n",
    "# Update with your API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')# Set your LangSmith API key as an environment variable.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # Set your OpenAI API key as an environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59846ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff62061d-0fb9-4ba9-b185-ff8c9746fb72",
   "metadata": {},
   "source": [
    "### 4. Create an Evaluation Dataset\n",
    "\n",
    "An evaluation dataset is a crucial component for testing any LLM application. It consists of a collection of examples, where each example contains:\n",
    "- **Inputs**: The data that will be fed into your model (e.g., a user's prompt).\n",
    "- **Outputs (Reference Labels)**: The corresponding \"ground truth\" or expected answer that you want the model to produce.\n",
    "\n",
    "Here, we will create a small dataset named `\"Oracle of Exactness\"` directly in LangSmith. It will contain two examples designed to test for precise outputs. We first check if the dataset already exists to avoid creating duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8ca802-e306-4632-afec-e9d655c84982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith # Import the LangSmith client library.\n",
    "\n",
    "client = langsmith.Client() # Instantiate the LangSmith client to interact with the platform.\n",
    "dataset_name = \"Oracle of Exactness\" # Define a name for our new dataset.\n",
    "\n",
    "# Check if a dataset with this name already exists in your LangSmith project.\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    # If the dataset does not exist, create it.\n",
    "    ds = client.create_dataset(dataset_name)\n",
    "    # Add examples to the newly created dataset.\n",
    "    client.create_examples(\n",
    "        # 'inputs' is a list of dictionaries, each representing an input to the model.\n",
    "        inputs=[\n",
    "            {\n",
    "                \"prompt_template\": \"State the year of the declaration of independence. Respond with just the year in digits, nothign else\"\n",
    "            },\n",
    "            {\"prompt_template\": \"What's the average speed of an unladen swallow?\"},\n",
    "        ],\n",
    "        # 'outputs' is a list of dictionaries with the corresponding expected or ground-truth answers.\n",
    "        outputs=[{\"output\": \"1776\"}, {\"output\": \"5\"}],\n",
    "        # 'dataset_id' links these examples to the dataset we created above.\n",
    "        dataset_id=ds.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5ea231-7901-44b8-9d66-761d3aca140a",
   "metadata": {},
   "source": [
    "### 5. Define the System and Evaluators\n",
    "\n",
    "Now we'll set up the components needed to run the evaluation. This involves three key parts:\n",
    "\n",
    "1.  **The System Under Test (`predict_result`)**: This is the function that we want to evaluate. It takes an input dictionary (matching the structure of our dataset inputs), uses an OpenAI model to generate a response, and returns the result in a structured output dictionary.\n",
    "\n",
    "2.  **A Custom Evaluator (`compare_label`)**: While LangSmith provides a built-in `\"exact_match\"` evaluator, we define our own here to demonstrate how you can create custom evaluation logic. This function receives the model's output (`run`) and the ground truth data (`example`), compares them, and returns a structured `EvaluationResult`. The `@run_evaluator` decorator registers this function with LangSmith so it can be used in an evaluation run.\n",
    "\n",
    "3.  **The Evaluation Configuration (`RunEvalConfig`)**: This object bundles all the evaluators we want to apply to each model prediction. We include both LangSmith's pre-built `\"exact_match\"` evaluator and our custom `compare_label` function. This will allow us to see their results side-by-side and confirm they produce the same scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad3e9fc-72ac-4854-a67a-378ae0c8c91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'bold-sink-95' at:\n",
      "https://smith.langchain.com/o/0212d326-bd9d-42bb-9937-c063f40f2361/datasets/b0ea32a6-a6bc-4ee5-94a1-f0847d0c3fc8/compare?selectedSessions=eb5cec96-dd0f-4b46-9485-04ac4b17da94\n",
      "\n",
      "View all tests for Dataset Oracle of Exactness at:\n",
      "https://smith.langchain.com/o/0212d326-bd9d-42bb-9937-c063f40f2361/datasets/b0ea32a6-a6bc-4ee5-94a1-f0847d0c3fc8\n",
      "[------------------------------------------------->] 2/2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'bold-sink-95',\n",
       " 'results': {'772c051d-c8ae-4438-a03f-97de3fb95d41': {'input': {'prompt_template': 'State the year of the declaration of independence. Respond with just the year in digits, nothign else'},\n",
       "   'feedback': [EvaluationResult(key='exact_match', score=1, value=None, comment=None, correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('20673b4c-b85a-4252-b1b9-2ca64e82f061'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None),\n",
       "    EvaluationResult(key='matches_label', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ba7767c4-ed36-410f-880a-b5731d50b45c'), target_run_id=None, extra=None)],\n",
       "   'execution_time': 1.206973,\n",
       "   'run_id': '6f05b482-937e-4667-8f42-75fc62346dc3',\n",
       "   'output': {'output': '1776'},\n",
       "   'reference': {'output': '1776'}},\n",
       "  '843f6ddf-d63d-492b-af9d-a120d8169489': {'input': {'prompt_template': \"What's the average speed of an unladen swallow?\"},\n",
       "   'feedback': [EvaluationResult(key='exact_match', score=0, value=None, comment=None, correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1100bcec-b83d-4255-b4ed-89bb3300d832'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None),\n",
       "    EvaluationResult(key='matches_label', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('7244cf39-3944-4954-bd9b-aedd2ec1566b'), target_run_id=None, extra=None)],\n",
       "   'execution_time': 1.426188,\n",
       "   'run_id': '613efe0b-d541-4e88-81c0-aa4f441ea952',\n",
       "   'output': {'output': 'The average speed of an unladen European Swallow is estimated to be around 20.1 miles per hour or 32.4 kilometers per hour.'},\n",
       "   'reference': {'output': '5'}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig # Import the configuration class for evaluation runs.\n",
    "from langchain_openai import ChatOpenAI # Import the ChatOpenAI class to interact with OpenAI's chat models.\n",
    "from langsmith.evaluation import EvaluationResult, run_evaluator # Import classes for creating custom evaluators.\n",
    "\n",
    "model = \"gpt-3.5-turbo\" # Specify the OpenAI model we want to use for our test.\n",
    "\n",
    "\n",
    "# This is your model/system that you want to evaluate.\n",
    "def predict_result(input_: dict) -> dict:\n",
    "    # This function calls the OpenAI model with the provided prompt.\n",
    "    response = ChatOpenAI(model=model).invoke(input_[\"prompt_template\"])\n",
    "    # It then returns the model's output in the standard dictionary format.\n",
    "    return {\"output\": response.content}\n",
    "\n",
    "\n",
    "# The '@run_evaluator' decorator registers this function as a LangSmith evaluator.\n",
    "@run_evaluator\n",
    "def compare_label(run, example) -> EvaluationResult:\n",
    "    # Custom evaluators let you define how \"exact\" the match ought to be.\n",
    "    # 'run' contains information about the model's execution, including its outputs.\n",
    "    # 'example' contains information from the dataset, including the reference output.\n",
    "    \n",
    "    # Flexibly pick the fields to compare by accessing the dictionaries.\n",
    "    prediction = run.outputs.get(\"output\") or \"\" # Get the predicted output string from the run, defaulting to an empty string if not found.\n",
    "    target = example.outputs.get(\"output\") or \"\" # Get the target (reference) output string from the example.\n",
    "    \n",
    "    # Perform the direct string comparison.\n",
    "    match = prediction and prediction == target\n",
    "    \n",
    "    # Return the result in the required EvaluationResult format.\n",
    "    return EvaluationResult(key=\"matches_label\", score=match)\n",
    "\n",
    "\n",
    "# This defines how you generate metrics about the model's performance.\n",
    "eval_config = RunEvalConfig(\n",
    "    # Specify a list of built-in evaluators. `\"exact_match\"` performs the same logic as our custom one.\n",
    "    evaluators=[\"exact_match\"], \n",
    "    # Specify a list of custom evaluator functions to run.\n",
    "    custom_evaluators=[compare_label],\n",
    ")\n",
    "\n",
    "# This is the main function that executes the evaluation.\n",
    "client.run_on_dataset(\n",
    "    dataset_name=dataset_name, # The name of the dataset in LangSmith to use for evaluation.\n",
    "    llm_or_chain_factory=predict_result, # A reference to the function/chain that will be tested.\n",
    "    evaluation=eval_config, # The evaluation configuration object we defined above.\n",
    "    verbose=True, # Prints progress and links to the results in LangSmith.\n",
    "    # Add any metadata to the project to help with tracking and organization.\n",
    "    project_metadata={\"version\": \"1.0.0\", \"model\": model},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
