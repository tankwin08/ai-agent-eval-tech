{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a7184af-d54f-487d-ad7f-0f3274dc689b",
   "metadata": {},
   "source": [
    "# RAG Evaluation using Fixed Sources\n",
    "\n",
    "====================================\n",
    "\n",
    "### 1. Theory: Component-wise Evaluation for RAG Systems\n",
    "\n",
    "A Retrieval-Augmented Generation (RAG) pipeline is a multi-stage system, typically composed of at least two core components: a **Retriever** (which fetches relevant documents) and a **Response Generator** (which synthesizes an answer based on those documents). While end-to-end evaluation is useful for measuring the overall performance, it can be difficult to pinpoint the source of errors. Is a bad answer due to poor retrieval or a faulty generation?\n",
    "\n",
    "To get more actionable insights, it's highly beneficial to evaluate each component in isolation. This walkthrough focuses on a key technique for evaluating the **Response Generator**. The strategy is to \"fix\" the source documents for each question, thereby removing the retriever from the equation. We can then measure how well the generator performs its specific task: answering a question faithfully based on a given context.\n",
    "\n",
    "We will create a dataset where the inputs include both the `question` and the `documents` the generator should use. We will then evaluate the generator on two criteria:\n",
    "1.  **Correctness**: Is the final answer factually correct, based on a reference label?\n",
    "2.  **Faithfulness**: Is the answer *only* based on the provided documents, without hallucinating or using outside knowledge?\n",
    "\n",
    "The final evaluation results in LangSmith will look something like this:\n",
    "\n",
    "![Custom Evaluator](./img/example_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "### 2. Prerequisites and Setup\n",
    "\n",
    "First, we'll install the necessary Python packages and configure our environment variables to connect to LangSmith and the model providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c748b92-e590-408f-bd20-733dc79d643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '%pip install' command installs python packages from the notebook.\n",
    "# -U flag ensures we get the latest versions.\n",
    "# %pip install -U langchain openai anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c086f0-f1c4-4a55-a922-c926239de2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system.\n",
    "import uuid # Import the uuid library to generate unique identifiers.\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint.\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"  # Update with your API key.\n",
    "uid = uuid.uuid4() # Generate a unique ID to keep dataset names unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7ab49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # Import function to load environment variables\n",
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Load environment variables from the .env file. The `override=True` argument\n",
    "# ensures that variables from the .env file will overwrite existing environment variables.\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint as an environment variable.\n",
    "# Update with your API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')# Set your LangSmith API key as an environment variable.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # Set your OpenAI API key as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a0309-48f8-4770-8b34-2b97eb85a247",
   "metadata": {},
   "source": [
    "### Step 1: Create a Dataset with Fixed Context\n",
    "\n",
    "We will now create our evaluation dataset. The key feature of this dataset is its structure. For each example, the `inputs` dictionary will contain both the user's `question` and a list of `documents`. This list of documents represents the fixed context that our response generator will use. The `outputs` dictionary contains the `label`, which is the ground-truth answer for the correctness check.\n",
    "\n",
    "The examples below are designed to test if the response generator can correctly extract information and whether it will ignore its pre-trained knowledge in favor of the provided (and sometimes counter-intuitive) context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f83f2e-76d1-4d86-9275-35bd61df014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple example dataset to illustrate the concept.\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"What's the company's total revenue for q2 of 2022?\",\n",
    "            # The 'documents' are part of the input for the component we are testing.\n",
    "            \"documents\": [\n",
    "                {\n",
    "                    \"metadata\": {},\n",
    "                    \"page_content\": \"In q1 the lemonade company made $4.95. In q2 revenue increased by a sizeable amount to just over $2T dollars.\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            # The 'label' is the ground-truth answer for correctness evaluation.\n",
    "            \"label\": \"2 trillion dollars\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\n",
    "            \"question\": \"Who is Lebron?\",\n",
    "            # This document provides a fictional, counter-intuitive context.\n",
    "            \"documents\": [\n",
    "                {\n",
    "                    \"metadata\": {},\n",
    "                    \"page_content\": \"On Thursday, February 16, Lebron James was nominated as President of the United States.\",\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "        \"outputs\": {\n",
    "            \"label\": \"Lebron James is the President of the USA.\",\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf85a45-e100-4d28-a102-ec1d135f0ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['4599fd5f-c2b4-43da-9c0e-b10f6dfc7f46',\n",
       "  'c3ea196b-e756-4e4e-9a44-670c2ffa183c'],\n",
       " 'count': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client # Import the Client class to interact with LangSmith.\n",
    "\n",
    "client = Client() # Instantiate the LangSmith client.\n",
    "\n",
    "dataset_name = f\"Faithfulness Example - {uid}\" # Create a unique name for our dataset.\n",
    "dataset = client.create_dataset(dataset_name=dataset_name) # Create the dataset on the LangSmith platform.\n",
    "# Create the examples in the dataset.\n",
    "client.create_examples(\n",
    "    inputs=[e[\"inputs\"] for e in examples], # Pass the list of input dictionaries.\n",
    "    outputs=[e[\"outputs\"] for e in examples], # Pass the list of output dictionaries.\n",
    "    dataset_id=dataset.id, # Link these examples to the dataset we just created.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa0264-d24f-495a-b9f6-87ddf97aaeb6",
   "metadata": {},
   "source": [
    "## Step 2: Define the Chain Component\n",
    "\n",
    "Next, we define our RAG system. We'll show the full chain for context, but we will clearly separate the **`response_synthesizer`** component. This synthesizer is the specific part of the chain that we will be evaluating. It takes a dictionary containing `documents` and a `question` and generates the final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6314168f-9530-476f-949b-d49c40db55ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import chat_models, prompts # Import core LangChain components.\n",
    "from langchain_core.documents import Document # Import the Document class.\n",
    "from langchain_core.retrievers import BaseRetriever # Import the base retriever class.\n",
    "from langchain_core.runnables import RunnablePassthrough # Import a passthrough runnable.ÃŸ\n",
    "from langchain_openai import ChatOpenAI # OpenAI chat model wrapper.\n",
    "\n",
    "# This is a placeholder retriever to illustrate the full chain. It will not be used in our evaluation.\n",
    "class MyRetriever(BaseRetriever):\n",
    "    def _get_relevant_documents(self, query, *, run_manager):\n",
    "        return [Document(page_content=\"Example\")]\n",
    "\n",
    "model = \"gpt-3.5-turbo\"\n",
    "# This is the specific component we will be evaluating.\n",
    "response_synthesizer = prompts.ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Respond using the following documents as context:\\n{documents}\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ") | ChatOpenAI(model=model, max_tokens=1000) # We pipe the prompt to an LLM.\n",
    "\n",
    "# The full RAG chain is shown below for illustrative purposes only.\n",
    "chain = {\n",
    "    \"documents\": MyRetriever(),\n",
    "    \"qusetion\": RunnablePassthrough(),\n",
    "} | response_synthesizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6087fa-432d-4a59-b023-29058b5ec6ea",
   "metadata": {},
   "source": [
    "## Step 3: Define a Custom Faithfulness Evaluator\n",
    "\n",
    "To measure faithfulness, we need an evaluator that checks if the model's `prediction` is consistent with the provided `documents`. Standard evaluators assume the reference context comes from the `outputs` of a dataset example. In our case, the context (the documents) is in the `inputs`.\n",
    "\n",
    "To handle this, we'll create a custom `FaithfulnessEvaluator`. This class will wrap a standard LangChain scoring evaluator but will override the data mapping. It will tell the underlying evaluator to use:\n",
    "- The model's generation as the `prediction`.\n",
    "- The `question` from the run's inputs as the `input`.\n",
    "- The `documents` from the *example's inputs* as the `reference` context.\n",
    "\n",
    "This allows us to use an off-the-shelf LLM-based scoring mechanism with our custom dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8217e940-e6d0-4f08-bed7-41cda7a35ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import RunEvaluator, EvaluationResult # Import the base classes for custom evaluation.\n",
    "from langchain.evaluation import load_evaluator # Import a helper to load built-in evaluators.\n",
    "\n",
    "\n",
    "# Define our custom evaluator class, inheriting from RunEvaluator.\n",
    "class FaithfulnessEvaluator(RunEvaluator):\n",
    "    def __init__(self):\n",
    "        # Initialize a built-in 'labeled_score_string' evaluator.\n",
    "        # This evaluator uses an LLM to score a prediction on a 1-10 scale based on given criteria.\n",
    "        self.evaluator = load_evaluator(\n",
    "            \"labeled_score_string\",\n",
    "            criteria={\n",
    "                \"faithful\": \"How faithful is the submission to the reference context?\"\n",
    "            },\n",
    "            normalize_by=10, # Normalize the score to be between 0 and 1.\n",
    "        )\n",
    "\n",
    "    # This is the core method that LangSmith will call for each run.\n",
    "    def evaluate_run(self, run, example) -> EvaluationResult:\n",
    "        # Call the underlying evaluator's 'evaluate_strings' method with custom-mapped fields.\n",
    "        res = self.evaluator.evaluate_strings(\n",
    "            prediction=next(iter(run.outputs.values())), # The LLM's generated answer.\n",
    "            input=run.inputs[\"question\"], # The user's question.\n",
    "            # This is the key part: we use the 'documents' from the example's INPUTS as the reference context.\n",
    "            reference=str(example.inputs[\"documents\"]),\n",
    "        )\n",
    "        # Return the result in the standard EvaluationResult format.\n",
    "        return EvaluationResult(key=\"labeled_criteria:faithful\", **res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-e7f8-f9a0-b1c2d3e4f5a6",
   "metadata": {},
   "source": [
    "### Step 4: Run the Evaluation\n",
    "\n",
    "Now we can run the evaluation. We will configure it to use two evaluators:\n",
    "1. The standard `\"qa\"` evaluator, which will measure correctness against the `label` in our dataset outputs.\n",
    "2. Our custom `FaithfulnessEvaluator`, which will measure how grounded the response is in the provided documents.\n",
    "\n",
    "We will pass the `response_synthesizer` directly as the system to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70e56fe8-ae02-481d-b6f6-729e535c5e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'slight-cat-16' at:\n",
      "https://smith.langchain.com/o/0212d326-bd9d-42bb-9937-c063f40f2361/datasets/6f4aa3b5-0f2b-4490-a3f0-66f09360fe81/compare?selectedSessions=eb5508b6-322f-48e9-b9ff-778d77bcde49\n",
      "\n",
      "View all tests for Dataset Faithfulness Example - 84799d49-fed3-44f8-aeab-4b72b546db4d at:\n",
      "https://smith.langchain.com/o/0212d326-bd9d-42bb-9937-c063f40f2361/datasets/6f4aa3b5-0f2b-4490-a3f0-66f09360fe81\n",
      "[------------------------------------------------->] 2/2\n"
     ]
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig # Import the evaluation configuration class.\n",
    "\n",
    "# Create an evaluation configuration.\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\"qa\"], # Include the standard 'qa' correctness evaluator.\n",
    "    custom_evaluators=[FaithfulnessEvaluator()], # Include our custom faithfulness evaluator.\n",
    "    input_key=\"question\", # Tell the 'qa' evaluator to use the 'question' field from the inputs.\n",
    ")\n",
    "# Run the evaluation on the dataset.\n",
    "results = client.run_on_dataset(\n",
    "    llm_or_chain_factory=response_synthesizer, # The specific component to be tested.\n",
    "    dataset_name=dataset_name, # The name of our dataset in LangSmith.\n",
    "    evaluation=eval_config, # The evaluation configuration.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e74b5b-b8e2-433d-a9da-c56539152833",
   "metadata": {},
   "source": [
    "You can now review the results in LangSmith by clicking the link in the output above. You will see scores for both correctness (`qa`) and faithfulness (`labeled_criteria:faithful`). Inspecting the trace for the faithfulness evaluator will show how the LLM judged the response against the provided documents.\n",
    "\n",
    "[![](./img/example_score.png)](https://smith.langchain.com/public/9a4e6ee2-f26c-4bcd-a050-04766fbfd350/r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca9286b-0463-4f81-a27b-b2e3b16955c2",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "You have now successfully evaluated a RAG system's response generator in isolation, testing both its **correctness** and its **faithfulness** to the provided context. This is an effective way to debug and improve a specific component of your RAG pipeline.\n",
    "\n",
    "The key technical insight was the use of a custom `RunEvaluator`. While most of LangChain's built-in evaluators are `StringEvaluator`s, which have a rigid expectation of where to find the `input`, `prediction`, and `reference` strings, the `RunEvaluator` interface gives you full control. It provides access to the entire `Run` and `Example` objects, allowing you to flexibly map any field from your run traces or dataset to the inputs of an underlying evaluator.\n",
    "\n",
    "In our case, we used this flexibility to take the reference context (`documents`) from the example's **inputs**, whereas a standard `StringEvaluator` would have looked for it in the example's `outputs`. This powerful pattern enables a wide range of custom evaluation strategies tailored to your specific application and dataset structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents-eval-techniques (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
