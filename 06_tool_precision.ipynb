{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6344e99-4dd7-4898-9a9d-516a1076ed54",
   "metadata": {},
   "source": [
    "# Tool Selection Evaluation\n",
    "\n",
    "====================================\n",
    "\n",
    "### 1. Theory: The Importance of Tool Selection\n",
    "\n",
    "Modern LLM-powered agents gain their power by interacting with the outside world through a set of tools (e.g., APIs, functions, databases). **Tool selection** is the critical process where the agent, based on a user's query, decides which tool (or tools) is most appropriate to use to generate a response. The quality of this selection is paramount to the agent's effectiveness.\n",
    "\n",
    "The agent's ability to choose the right tool depends almost entirely on the quality of the tool's **name and description**. A vague or misleading description will cause the agent to make mistakes, leading to incorrect actions, wasted API calls, and poor user experiences.\n",
    "\n",
    "This notebook provides a complete walkthrough of how to:\n",
    "1.  **Evaluate** an agent's tool selection capability using a custom precision metric.\n",
    "2.  **Automatically improve** the tool descriptions by using another LLM to analyze the failure cases.\n",
    "3.  **Re-evaluate** to see if the improvements had a positive effect.\n",
    "4.  **Validate** the results on a held-out test set to ensure the improvements are generalizable.\n",
    "\n",
    "We will use a subset of the [ToolBench](https://github.com/OpenBMB/ToolBench/tree/master) dataset, which is specifically designed for this kind of task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "### 2. Prerequisites and Setup\n",
    "\n",
    "First, we'll install the necessary Python packages for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865947b4-9ce1-4a76-96bc-76a656abe8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The '%pip install' command installs python packages from the notebook.\n",
    "# # The -U flag ensures we get the latest versions of the langchain and openai libraries.\n",
    "# %pip install -U langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-b8c9-d0e1-f2a3b4c5d6e7",
   "metadata": {},
   "source": [
    "Next, we configure our environment variables. This is a secure way to provide API keys to our application.\n",
    "\n",
    "- **`LANGCHAIN_API_KEY`**: Your secret key for authenticating with LangSmith.\n",
    "- **`LANGCHAIN_ENDPOINT`**: This URL directs all LangChain tracing data to the LangSmith platform.\n",
    "- **`LANGCHAIN_PROJECT`**: (Optional) This sets the project name in LangSmith, helping to organize your runs. If not set, it defaults to `\"default\"`.\n",
    "\n",
    "**Action Required**: You must replace `\"YOUR API KEY\"` with your actual key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72e5062b-91e5-4a8e-9d46-4ebfa53ec280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint.\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"  # Update with your API key.\n",
    "# Optional: Set a project name in LangSmith to group related runs.\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Tool Selection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ab0aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # Import function to load environment variables\n",
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Load environment variables from the .env file. The `override=True` argument\n",
    "# ensures that variables from the .env file will overwrite existing environment variables.\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint as an environment variable.\n",
    "# Update with your API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')# Set your LangSmith API key as an environment variable.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # Set your OpenAI API key as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-a7b8-c9d0-e1f2-a3b4c5d6e7f8",
   "metadata": {},
   "source": [
    "### 3. Prepare the Dataset\n",
    "\n",
    "We will use a public dataset hosted on LangSmith. This dataset contains examples where the input is a user query, and the output is the list of tools that are expected to be called. We will clone this dataset into our own LangSmith account so we can run evaluations against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1a4652-11dd-4c9c-b882-05def42115ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL of the public LangSmith dataset we will use for development and initial evaluation.\n",
    "dev_dataset_url = (\n",
    "    \"https://smith.langchain.com/public/bdf7611c-3420-4c71-a492-42715a32d61e/d\"\n",
    ")\n",
    "# A descriptive name for our local copy of the dataset.\n",
    "dataset_name = \"Tool Selection (Logistics) dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f951dbc-01d9-4482-a51f-095ce0660f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(name='Tool Selection (Logistics) dev', description=None, data_type=<DataType.kv: 'kv'>, id=UUID('63c7d3fa-cffb-4a3e-832c-e5ba400b0792'), created_at=datetime.datetime(2025, 8, 5, 15, 16, 54, 370560, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 8, 5, 15, 16, 54, 370560, tzinfo=datetime.timezone.utc), example_count=0, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None, transformations=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langsmith # Import the langsmith library.\n",
    "\n",
    "client = langsmith.Client() # Instantiate the LangSmith client.\n",
    "\n",
    "# Use the client to clone the public dataset into your own LangSmith workspace.\n",
    "client.clone_public_dataset(dev_dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff0ea7-0f9c-4f6c-aeae-d9361dae5e8f",
   "metadata": {},
   "source": [
    "### 4. Define Metrics\n",
    "\n",
    "To measure how well the agent selects tools, we need a quantitative metric. We will calculate the **precision** of the selected tools. In this context, precision answers the question: *\"Of all the tools the agent chose, what fraction were correct?\"*\n",
    "\n",
    "We will implement this as a custom evaluator in LangSmith. The logic is as follows:\n",
    "\n",
    "1. Get the set of predicted tools and the set of expected tools.\n",
    "2. Find the intersection of these two sets, which gives us the \"True Positives\".\n",
    "3. Calculate Precision = (Number of True Positives) / (Total Number of Predicted Tools).\n",
    "\n",
    "This gives us a score between 0 and 1 for each run, which we can then average across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a18b5b77-9000-4eb1-8b34-d4aa84708479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set # Import the Set type hint.\n",
    "\n",
    "from langchain.smith import RunEvalConfig # Import the evaluation configuration class.\n",
    "from langsmith.evaluation import run_evaluator # Import the decorator for creating custom evaluators.\n",
    "\n",
    "\n",
    "# The '@run_evaluator' decorator registers this function as a custom evaluator in LangSmith.\n",
    "@run_evaluator\n",
    "def selected_tools_precision(run, example):\n",
    "    # Get the expected tool calls from the dataset example.\n",
    "    expected = example.outputs[\"expected\"]\n",
    "    # Get the predicted tool calls from the agent's run output.\n",
    "    predicted = run.outputs[\"output\"]\n",
    "    # The expected format is a list of lists; flatten it and convert to a set for efficient operations.\n",
    "    expected: Set[str] = {tool for tools in expected for tool in tools}\n",
    "    # The predicted format is a list of dictionaries; extract the tool name ('type') and convert to a set.\n",
    "    predicted: Set[str] = {tool[\"type\"] for tool in predicted}\n",
    "    # Find the common tools between predicted and expected sets (the intersection).\n",
    "    true_positives = predicted & expected\n",
    "\n",
    "    # Handle the edge case where no tools were predicted.\n",
    "    if len(predicted) == 0:\n",
    "        if len(expected) > 0:\n",
    "            score = 0 # It should have predicted tools but didn't.\n",
    "        else:\n",
    "            score = 1 # It correctly predicted no tools.\n",
    "    else:\n",
    "        # Calculate precision: the ratio of correct predictions to total predictions.\n",
    "        score = len(true_positives) / len(predicted)\n",
    "    # Return the score in the format LangSmith requires.\n",
    "    return {\"key\": \"tool_selection_precision\", \"score\": score}\n",
    "\n",
    "\n",
    "# Create an evaluation configuration object that includes our custom evaluator.\n",
    "eval_config = RunEvalConfig(\n",
    "    custom_evaluators=[selected_tools_precision],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc664e2-d88d-444c-9bc5-4b60a9d105dc",
   "metadata": {},
   "source": [
    "### 5. Create the Function-Calling Model (V1)\n",
    "\n",
    "We will now create the agent, which in this case is a simple function-calling chain. The tools are defined in an external JSON file, which we will load. We then use LangChain's `.bind_tools()` method to make the LLM aware of the available tools and their schemas. This is a streamlined way to create a tool-using agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce123ef5-c8fe-4522-84e7-2790ce8e8e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'TransportistasdeArgentina',\n",
       "  'description': 'Quote for postcode in OCA e-Pack.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'postCodeDst': {'type': 'number',\n",
       "     'description': 'Postcode Destination'},\n",
       "    'cuit': {'type': 'string',\n",
       "     'description': 'CUIT of your account in OCA e-Pack'},\n",
       "    'operativa': {'type': 'string',\n",
       "     'description': 'Operativa number of your account in OCA e-Pack'},\n",
       "    'cost': {'type': 'number', 'description': 'Cost of products in ARS'},\n",
       "    'postCodeSrc': {'type': 'number', 'description': 'Postcode Source'},\n",
       "    'volume': {'type': 'number', 'description': 'Volume in cm3'},\n",
       "    'weight': {'type': 'number', 'description': 'Weight in KG'}},\n",
       "   'required': ['postCodeDst',\n",
       "    'cuit',\n",
       "    'operativa',\n",
       "    'cost',\n",
       "    'postCodeSrc',\n",
       "    'volume',\n",
       "    'weight']}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json # Import the json library for file handling.\n",
    "\n",
    "# Open the JSON file containing the tool definitions.\n",
    "with open(\"./data/tools.json\") as f:\n",
    "    # Load the JSON content into a Python list.\n",
    "    tools = json.load(f)\n",
    "\n",
    "# Display the first tool as an example of the structure.\n",
    "tools[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58935649-8aa6-47ea-a64d-84122e6f6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.openai_tools import JsonOutputToolsParser # Parser for the tool-calling output.\n",
    "from langchain_core.prompts import ChatPromptTemplate # Prompt templating utility.\n",
    "from langchain_openai import ChatOpenAI # OpenAI chat model wrapper.\n",
    "\n",
    "model = \"gpt-3.5-turbo\" # Specify the model to use.\n",
    "\n",
    "# Define a simple prompt template for the assistant.\n",
    "assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Respond to the user's query using the provided tools\",\n",
    "        ),\n",
    "        (\"user\", \"{query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the LLM and bind the tools to it. This tells the LLM which functions it can call.\n",
    "llm = ChatOpenAI(model=model).bind_tools(tools)\n",
    "\n",
    "# Create the final chain using LangChain Expression Language (LCEL).\n",
    "chain = assistant_prompt | llm | JsonOutputToolsParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa4c04-5b0f-4126-9d83-aad3de2740fd",
   "metadata": {},
   "source": [
    "### 6. Evaluate (V1)\n",
    "\n",
    "Now we run our first evaluation, testing our initial chain against the development dataset. LangSmith will orchestrate the process, and our custom `selected_tools_precision` evaluator will score each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75d194a4-0cd1-4b34-836f-ef4b5c405a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'clear-jet-37' at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/462d8386-60c8-4cb3-84eb-6efeae3a1293/compare?selectedSessions=8b95a94e-c05f-4ecf-b749-aeaef3ff3327\n",
      "\n",
      "View all tests for Dataset Tool Selection (Logistics) dev at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/462d8386-60c8-4cb3-84eb-6efeae3a1293\n",
      "[------------------------------------------------->] 100/100"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.tool_selection_precision</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>827e2f98-bcb1-4940-aa16-5a7d0eca80ff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.636667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.417737</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.370322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.581734</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.468482</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.141958</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.331713</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.576078</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.320643</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.tool_selection_precision error  execution_time  \\\n",
       "count                          100.000000     0      100.000000   \n",
       "unique                                NaN     0             NaN   \n",
       "top                                   NaN   NaN             NaN   \n",
       "freq                                  NaN   NaN             NaN   \n",
       "mean                             0.636667   NaN        1.417737   \n",
       "std                              0.370322   NaN        0.581734   \n",
       "min                              0.000000   NaN        0.468482   \n",
       "25%                              0.333333   NaN        1.141958   \n",
       "50%                              0.500000   NaN        1.331713   \n",
       "75%                              1.000000   NaN        1.576078   \n",
       "max                              1.000000   NaN        4.320643   \n",
       "\n",
       "                                      run_id  \n",
       "count                                    100  \n",
       "unique                                   100  \n",
       "top     827e2f98-bcb1-4940-aa16-5a7d0eca80ff  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the evaluation on the dataset.\n",
    "test_results = client.run_on_dataset(\n",
    "    dataset_name=dataset_name, # The name of our development dataset.\n",
    "    llm_or_chain_factory=chain, # The agent chain to be tested.\n",
    "    evaluation=eval_config, # The evaluation configuration with our custom evaluator.\n",
    "    verbose=True, # Print progress and links to the results.\n",
    "    project_metadata={\n",
    "        \"model\": model, # Tag the run with the model name.\n",
    "        \"tool_variant\": 0, # Tag the run with our tool description version.\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064eb29-dd91-4d63-a540-dd8f6c616ddd",
   "metadata": {},
   "source": [
    "After evaluating, the best practice is to manually review the failure cases in LangSmith, identify patterns, and thoughtfully update the tool descriptions or ground-truth labels. The dataset we are using is noisy, so some labels may be incorrect.\n",
    "\n",
    "For demonstration purposes, we will now show a more automated (but less reliable) approach to improving the tool descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8642e586-8e6a-4fd6-bd2e-1b47010758d2",
   "metadata": {},
   "source": [
    "### 7. Automated Improvement of Tool Descriptions\n",
    "\n",
    "Here, we'll use an LLM to try and fix its own mistakes. We will ask a second LLM to act as a \"documentation assistant\" that analyzes the failure cases from our first evaluation and suggests better tool descriptions. This process follows a **Map-Reduce-Distill** pattern:\n",
    "\n",
    "1.  **Map**: For each run that failed (precision < 1), we will *map* it to a suggested documentation update. We'll create an \"improver chain\" that takes the failure case (query, predicted tools, expected tools) and generates a new, improved description for the tool(s) it thinks were misused.\n",
    "2.  **Reduce**: We will group all the suggested description updates by tool name. A single tool might have multiple suggested improvements from different failure cases.\n",
    "3.  **Distill**: For each tool, we will use a final \"distill chain\" to synthesize the list of candidate descriptions into a single, cohesive, improved description.\n",
    "\n",
    "Let's start by defining the `improver_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4c3ea78-3665-4b4e-a7df-7f2849d65cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tankwin08/Desktop/projects/professional/ai-agents-eval-techniques/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/tankwin08/Desktop/projects/professional/ai-agents-eval-techniques/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1844: UserWarning: Received a Pydantic BaseModel V1 schema. This is not supported by method=\"json_schema\". Please use method=\"function_calling\" or specify schema via JSON Schema or Pydantic V2 BaseModel. Overriding to method=\"function_calling\".\n",
      "  warnings.warn(\n",
      "/Users/tankwin08/Desktop/projects/professional/ai-agents-eval-techniques/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1857: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import List # Import typing hints.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate # Prompt templating utility.\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field # Pydantic models for structured output.\n",
    "from langchain_core.runnables import chain as as_runnable # Runnable utilities.\n",
    "from langchain_openai import ChatOpenAI # OpenAI chat model wrapper.\n",
    "\n",
    "\n",
    "# Define the Pydantic schema for a single tool description update.\n",
    "class FunctionUpdate(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"The name of the tool whose description you'd like to update\"\n",
    "    )\n",
    "    updated_description: str = Field(\n",
    "        description=\"The updated description that would make it clear when and why to invoke this function.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the top-level Pydantic model for the LLM's structured output.\n",
    "class ImproveToolDocumentation(BaseModel):\n",
    "    \"\"\"Called to update the docstrings and other information about a given tool\n",
    "    so that the user has an easier time selecting.\"\"\"\n",
    "\n",
    "    updates: List[FunctionUpdate] = Field(\n",
    "        description=\"The updates to make, one for each tool description you'd like to change\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Define the prompt for the 'improver' LLM. It takes the full API list and a specific failure case.\n",
    "improver_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an API documentation assistant tasked with meticulously improving the descriptions of our API docs.\"\n",
    "            \" Our AI assistant is trying to assist users by calling APIs, but it continues to invoke the wrong ones.\"\n",
    "            \" You must improve their documentation to remove ambiguity so that the assistant will no longer make any mistakes.\\n\\n\"\n",
    "            \"##Valid APIs\\nBelow are the existing APIs the assistant is choosing between:\\n```apis.json\\n{apis}\\n```\\n\\n\"\n",
    "            \"## Failure Case\\nBelow is a user query, expected API calls, and actual API calls.\"\n",
    "            \" Use this failure case to make motivated doc changes.\\n\\n```failure_case.json\\n{failure}\\n```\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Respond with the updated tool descriptions to clear up\"\n",
    "            \" whatever ambiguity caused the failure case above.\"\n",
    "            \" Feel free to mention what it is NOT appropriate for (if that's causing issues.), like 'don't use this for x'.\"\n",
    "            \" The updated description should reflect WHY the assistant got it wrong in the first place.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize an LLM and configure it to produce structured output matching our Pydantic model.\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\").with_structured_output(ImproveToolDocumentation)\n",
    "\n",
    "# Create the final 'improver' chain.\n",
    "improver_chain = improver_prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4-b5c6-d7e8-f9a0-b1c2d3e4f5a6",
   "metadata": {},
   "source": [
    "Now, we process the results of our first evaluation to create the inputs for our `improver_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2b34a01-6c59-474d-a4ee-f92f48a8d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of all available tools to a JSON string for inclusion in the prompt.\n",
    "apis = json.dumps(tools, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8d7ced71-4d08-4da2-86ce-2cdaf0b5fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the test results into a pandas DataFrame for easy manipulation.\n",
    "df = test_results.to_dataframe()\n",
    "# Filter the DataFrame to only include rows where the tool selection was not perfect (score < 1).\n",
    "df = df[df[\"feedback.tool_selection_precision\"] < 1]\n",
    "\n",
    "\n",
    "# Define a function to format each row of the DataFrame into the input format for the improver_chain.\n",
    "def format_inputs(series):\n",
    "    return {\n",
    "        \"apis\": apis,\n",
    "        \"failure\": json.dumps(\n",
    "            {\n",
    "                \"query\": series[\"inputs.query\"],\n",
    "                \"predicted\": [out[\"type\"] for out in series[\"output\"]],\n",
    "                \"expected\": series[\"reference.expected\"][0],\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply the formatting function to each failure case to create a list of inputs.\n",
    "improver_inputs = df.apply(format_inputs, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf8d1c-8af4-4a16-afaf-189b50452554",
   "metadata": {},
   "source": [
    "#### Map errors to updates\n",
    "\n",
    "We run the `improver_chain` in a batch to efficiently generate suggested updates for all failure cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e35eabf9-d051-473c-a46d-e45c7da0cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the 'Map' step: run the improver_chain on all the failure cases.\n",
    "all_updates = improver_chain.batch(improver_inputs, return_exceptions=True)\n",
    "# Filter out any potential errors from the batch run to ensure we only have valid update objects.\n",
    "all_updates = [u for u in all_updates if isinstance(u, ImproveToolDocumentation)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74c0778-4b48-4599-b9af-7fb8f2dd4c41",
   "metadata": {},
   "source": [
    "#### Reduce updates per tool\n",
    "\n",
    "Next, we perform the 'Reduce' step by grouping all the suggested descriptions by the tool name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a125175-31fc-424d-8cfe-3cfaa3184281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict # Import defaultdict for easy aggregation.\n",
    "\n",
    "# Create a dictionary to hold lists of suggested updates for each tool.\n",
    "toolwise_updates = defaultdict(list)\n",
    "# Iterate through the list of all generated updates.\n",
    "for updates in all_updates:\n",
    "    # Each item can contain updates for multiple tools.\n",
    "    for tool_update in updates.updates:\n",
    "        # Append the suggested description to the list for that tool name.\n",
    "        toolwise_updates[tool_update.name].append(tool_update.updated_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcecf8bf-7bc2-4f6a-9aba-8cb6d814074b",
   "metadata": {},
   "source": [
    "#### Distill updates into a final description\n",
    "\n",
    "Finally, we perform the 'Distill' step. We create a new chain that takes the list of candidate descriptions for a single tool and synthesizes them into one final, improved description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f075478d-59f7-4521-aaaa-721350bc89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt for the 'distiller' LLM.\n",
    "distill_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an API documentation assistant tasked with meticulously improving the descriptions of our API docs.\"\n",
    "            \" Our AI assistant is trying to help users by calling APIs, but it continues to invoke the wrong ones.\"\n",
    "            \" You are tasked with updating the {target_api} description.\\n\\n\"\n",
    "            \"## Current APIs\\n\"\n",
    "            \"Below is a list of the current APIs and descriptions.\\n\"\n",
    "            \"```apis.json\\n{apis}\\n```\\n\\n\"\n",
    "            \"## Candidates\\n\"\n",
    "            \" Here are some candidate desription improvements:\\n{candidates}\\n\"\n",
    "            \" Consider the above feedback in your updated description.\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Respond with the updated description for the {target_api} API.\"\n",
    "            \" It should distill and incorporate the candidate descriptions to\"\n",
    "            \" clear up whatever ambiguity is causing our AI assistant to fail.\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(apis=apis) # Pre-fill the 'apis' part of the prompt.\n",
    "\n",
    "# Initialize an LLM configured to produce a single 'FunctionUpdate' object.\n",
    "distill_llm = ChatOpenAI(model=model).with_structured_output(FunctionUpdate)\n",
    "\n",
    "# Create the final 'distill' chain.\n",
    "distill_chain = distill_prompt | distill_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7ed363e-4e1e-4716-80dc-de3b099600ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the inputs for the distill_chain.\n",
    "distill_inputs = [\n",
    "    {\n",
    "        \"target_api\": name,\n",
    "        \"candidates\": \"\\n\".join([\"- \" + c for c in candidates]),\n",
    "    }\n",
    "    for name, candidates in toolwise_updates.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70b69ca7-20dd-433a-a1ed-cb1089c19212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the distill chain in a batch to generate the final descriptions for all tools.\n",
    "updated_descriptions = distill_chain.batch(distill_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4ddae40c-cf37-4295-a680-a41657e28ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TransportistasdeArgentina': 'Get a shipping quote for sending products within Argentina using OCA e-Pack. Provide destination and source postcodes, CUIT, operativa number, cost, volume, and weight details for accurate pricing.',\n",
       " 'TurkeyPostalCodes': 'Retrieve Turkish plate numbers (1 to 81) based on the city code. This API is specifically designed to provide details about Turkish plates and is not intended for tracking packages or obtaining postal codes for cities in Argentina.',\n",
       " 'CEPBrazil': 'Retrieve address details based on a Brazilian CEP number. This function is NOT intended for tracking package locations or statuses, tracking travel documents, or providing non-address related information. Use this API specifically for address lookup using CEP numbers in Brazil.',\n",
       " 'PridnestroviePost': 'Get track information by providing a track number for international shipments. Use this API specifically for tracking packages and shipments.',\n",
       " 'PackAndSend': 'If you have a Pack & Send Reference Number, use this API to track the delivery status and retrieve relevant information about the package. This API is specifically designed for tracking packages using the Pack & Send Reference Number, and it is not intended for providing postal code information for cities in a specific state.',\n",
       " 'TrackingMore_v2': 'List all supported carriers for package tracking. This API provides a comprehensive overview of available carriers for tracking packages. It is not intended for tracking specific package details, but rather for identifying carriers for package tracking services.',\n",
       " 'SQUAKE': 'This function does not have a defined purpose or parameters. Avoid using it as it does not serve any specific functionality. It is not suitable for tracking packages or retrieving package information.',\n",
       " 'AmexAustraliaFastwayAustraliaTracking': \"Track a package's shipping details specifically within Australia using a package tracking number. This API is designed for tracking packages shipped via the AmexAustraliaFastway service and is not suitable for tracking international shipments or non-package related inquiries.\",\n",
       " 'suivi-colis': 'Retrieve the current status (i.e., the latest status) of a package by providing the package ID. This function is suitable for tracking package statuses and obtaining real-time updates on delivery progress.',\n",
       " 'CreateContainerTracking': 'Retrieve data related to a container using the container ID provided. This API is suitable for tracking containers and retrieving their details.',\n",
       " 'Transitaires': 'Retrieve details about a specific transit company. This function is NOT designed for tracking packages or event planning.',\n",
       " 'KargomNerede': 'Retrieve a list of shipping companies.',\n",
       " 'GS1Parser': 'Parse machine- or human-readable GS1 barcode data.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the list of updated descriptions into a simple name: description dictionary.\n",
    "updates_dict = {upd.name: upd.updated_description for upd in updated_descriptions}\n",
    "# Display the final, improved descriptions.\n",
    "updates_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4-c5d6-e7f8-f9a0-b1c2d3e4f5a6",
   "metadata": {},
   "source": [
    "Now we create a new list of tools (`new_tools`) with our automatically generated descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd251899-4e78-456e-a49a-4efb312560e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy # Import deepcopy to create a new, independent copy of our tools list.\n",
    "\n",
    "# Create a deep copy of the original tools to avoid modifying them in place.\n",
    "new_tools = deepcopy(tools)\n",
    "# Iterate through the new list of tools.\n",
    "for tool in new_tools:\n",
    "    # Get the name of the current tool.\n",
    "    name = tool[\"function\"][\"name\"]\n",
    "    # Check if we have a generated an updated description for this tool.\n",
    "    if name in updates_dict:\n",
    "        # Get the new description.\n",
    "        updated = updates_dict[name]\n",
    "        # Overwrite the old description with the new, improved one.\n",
    "        tool[\"function\"][\"description\"] = updated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba119ad-63ea-4381-9ea5-10ff5d02f7cc",
   "metadata": {},
   "source": [
    "### 8. Re-Evaluate (V2)\n",
    "\n",
    "Now that we have our improved tool descriptions, we'll create a new agent chain (`updated_chain`) that uses them. We will then re-run the evaluation on the *same development dataset* to see if our automated changes led to a better precision score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3b909156-4667-45b3-aa97-a00e222106b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new LLM instance and bind our `new_tools` with the updated descriptions.\n",
    "llm = ChatOpenAI(model=model).bind_tools(new_tools)\n",
    "\n",
    "# Create the V2 chain, using the same prompt and parser but the new LLM.\n",
    "updated_chain = assistant_prompt | llm | JsonOutputToolsParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c7be884-e366-4878-9abf-44c83f402b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'ordinary-step-81' at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/462d8386-60c8-4cb3-84eb-6efeae3a1293/compare?selectedSessions=a4204d34-4d08-42fa-a84d-19b850ad920e\n",
      "\n",
      "View all tests for Dataset Tool Selection (Logistics) dev at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/462d8386-60c8-4cb3-84eb-6efeae3a1293\n",
      "[------------------------------------------------->] 99/100"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chain failed for example 033fd6d7-6c80-4ef2-ab26-e4116e4da24a with inputs {'query': \"I'm planning a family vacation to Brazil and I need to find a hotel in Rio de Janeiro. Can you provide me with a list of available hotels in Rio de Janeiro downtown? Additionally, I would like to know the current health status of the CEP Brazil API and if it's functioning properly.\"}\n",
      "Error Type: InternalServerError, Message: Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through our help center at help.openai.com if you keep seeing this error. (Please include the request ID req_35b74dde88208be45493f9827dc88674 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 100/100"
     ]
    }
   ],
   "source": [
    "model = \"gpt-3.5-turbo\" # Re-specify the model name.\n",
    "\n",
    "# Run the evaluation again with the updated chain.\n",
    "updated_test_results = client.run_on_dataset(\n",
    "    dataset_name=dataset_name, # Use the same development dataset.\n",
    "    llm_or_chain_factory=updated_chain, # Pass our new V2 chain.\n",
    "    evaluation=eval_config, # Use the same evaluation config.\n",
    "    project_metadata={\n",
    "        \"model\": model,\n",
    "        # Update the version number for our tool descriptions.\n",
    "        \"tool_variant\": 2,\n",
    "    },\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635ad65-4e6c-4404-8cbf-7c2d58183c03",
   "metadata": {},
   "source": [
    "The metrics show a slight improvement, though it may be within the standard margin of error. This automated approach is a good starting point, but manual analysis and refinement are often necessary for significant gains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a6ee0c-f0a9-4b55-b55a-38b18ee7b418",
   "metadata": {},
   "source": [
    "### 9. Final Validation on a Test Set\n",
    "\n",
    "So far, we have been developing and evaluating on a single dataset (our \"dev\" set). This process, often called **hill climbing**, can lead to **overfitting**: our new tool descriptions might be overly tailored to the specific examples in the dev set and may not perform as well on new, unseen data.\n",
    "\n",
    "To get a true, unbiased measure of whether our V2 chain is actually better than V1, we must benchmark both versions on a **held-out test set**. This is a set of data that was not used at all during the development and improvement process. A better score on this test set gives us high confidence that our changes have led to a genuinely more robust model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b4c73b35-b393-4408-bc28-cc7f6f43b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URLs for the different data splits (dev, test, train).\n",
    "dataset_urls = {\n",
    "    # Dev is the same as the one we've been using.\n",
    "    \"dev\": dev_dataset_url,\n",
    "    # The URL for the held-out test set.\n",
    "    \"test\": \"https://smith.langchain.com/public/a5fd6197-36ed-4d06-993a-89929dded399/d\",\n",
    "    # The URL for a training set (which could be used for fine-tuning).\n",
    "    \"train\": \"https://smith.langchain.com/public/cf5a1de8-68f0-4170-9bcc-f263c1abb063/d\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb0deeb7-59ae-4bd7-8329-b6efef8fef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langsmith # Import the langsmith library.\n",
    "\n",
    "client = langsmith.Client() # Instantiate the LangSmith client.\n",
    "\n",
    "# Clone the public test dataset into our workspace.\n",
    "client.clone_public_dataset(dataset_urls[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4351689-b051-4ba1-87ed-af9757ce7ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'definite-coach-89' at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/ddc1bcf7-c3fb-4669-824d-eb2e23af93d0/compare?selectedSessions=2b7204c8-7f07-4c2c-b798-d9005a059ce0\n",
      "\n",
      "View all tests for Dataset Tool Selection (Logistics) test at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/ddc1bcf7-c3fb-4669-824d-eb2e23af93d0\n",
      "[------------------------------------------------->] 234/234View the evaluation results for project 'sparkling-doctor-64' at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/ddc1bcf7-c3fb-4669-824d-eb2e23af93d0/compare?selectedSessions=b9d4bd07-d96b-4da8-97df-279158ffafa1\n",
      "\n",
      "View all tests for Dataset Tool Selection (Logistics) test at:\n",
      "https://smith.langchain.com/o/30239cd8-922f-4722-808d-897e1e722845/datasets/ddc1bcf7-c3fb-4669-824d-eb2e23af93d0\n",
      "[------------------------------------------------> ] 231/234"
     ]
    }
   ],
   "source": [
    "# Define the name of our newly cloned test dataset.\n",
    "test_dataset_name = \"Tool Selection (Logistics) test\"\n",
    "\n",
    "# Iterate through both the original chain (V1) and the updated chain (V2).\n",
    "for target_chain in [chain, updated_chain]:\n",
    "    # Run each chain on the test dataset.\n",
    "    client.run_on_dataset(\n",
    "        dataset_name=test_dataset_name,\n",
    "        llm_or_chain_factory=chain, # The chain to be tested.\n",
    "        evaluation=eval_config, # The evaluation configuration.\n",
    "        project_metadata={\n",
    "            \"model\": model,\n",
    "            # Mark that this is a new tool description version.\n",
    "            \"tool_variant\": 2,\n",
    "        },\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents-eval-techniques (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
