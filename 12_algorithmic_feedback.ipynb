{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bddb3342-ae12-4d6b-98d1-21cb0674c1d0",
   "metadata": {},
   "source": [
    "# Creating an Automated Feedback Pipeline with LangSmith\n",
    "\n",
    "====================================\n",
    "\n",
    "### 1. Theory: Automated Monitoring with Algorithmic Feedback\n",
    "\n",
    "While manual review of your LLM application's traces is valuable for deep debugging, it is not a scalable solution for monitoring a production system. To understand performance at scale, we need automated metrics. **Algorithmic feedback** is the process of programmatically generating quality scores for your application's runs *after* they have been completed. \n",
    "\n",
    "This is different from real-time feedback (which runs as a callback) and is typically run as a scheduled job (e.g., every hour or once a day) on a batch of recent production traces. By enriching your traces with these automated scores, you can create powerful monitoring dashboards in LangSmith to track metrics like relevance, verbosity, or correctness over time.\n",
    "\n",
    "![model-based feedback monitoring charts](./img/feedback_charts.png)\n",
    "\n",
    "If these metrics indicate a problem, you can easily filter for the low-scoring runs in LangSmith to identify problematic inputs, debug issues, or curate a dataset for fine-tuning.\n",
    "\n",
    "This tutorial will walk you through the process:\n",
    "\n",
    "1.  **Filter Runs**: Select a batch of completed runs from a LangSmith project that you want to score.\n",
    "2.  **Define Feedback Logic**: Create functions or chains that calculate your desired feedback metrics. We will show examples of both simple statistical metrics and more powerful AI-assisted metrics.\n",
    "3.  **Send Feedback to LangSmith**: Use the LangSmith client to attach the generated scores to the original runs.\n",
    "\n",
    "We'll be using the LangSmith and LangChain Hub, so make sure you have the necessary API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "### 2. Prerequisites and Setup\n",
    "\n",
    "First, we configure our environment variables. This is a secure way to provide API keys to our application.\n",
    "\n",
    "**Action Required**: You must replace the placeholder values with your actual keys and provide a project name to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f496bc2-d9c6-46ea-be19-98ff2dd2800c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# # Update with your API key\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR API KEY\"\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_HUB_API_URL\"] = \"https://api.hub.langchain.com\"\n",
    "# Update with your Hub API key\n",
    "os.environ[\"LANGCHAIN_HUB_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')\n",
    "# Change to the project name you want to add feedback to.\n",
    "project_name = \"12_algo_feedback\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09618d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # Import function to load environment variables\n",
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Load environment variables from the .env file. The `override=True` argument\n",
    "# ensures that variables from the .env file will overwrite existing environment variables.\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint as an environment variable.\n",
    "# Update with your API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')# Set your LangSmith API key as an environment variable.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # Set your OpenAI API key as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6a08dc-479e-4772-913d-5e4a9c057ff9",
   "metadata": {},
   "source": [
    "To ensure we have a common starting point, the following cell will create some example runs in the specified project. In a real-world scenario, you would be targeting runs generated by your live application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2edcc9b1-0c2c-4092-a354-0999ad1ef4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client # Import the LangSmith client.\n",
    "from datetime import datetime # Import the datetime module.\n",
    "\n",
    "client = Client() # Instantiate the client.\n",
    "# Define a list of example input/output pairs.\n",
    "example_data = [\n",
    "    (\"Who trained Llama-v2?\", \"I'm sorry, but I don't have that information.\"),\n",
    "    (\n",
    "        \"When did langchain first announce the hub?\",\n",
    "        \"LangChain first announced the LangChain Hub on September 5, 2023.\",\n",
    "    ),\n",
    "    (\n",
    "        \"What's LangSmith?\",\n",
    "        \"LangSmith is a platform developed by LangChain for building production-grade LLM (Language Model) applications. It allows you to debug, test, evaluate, and monitor chains and intelligent agents built on any LLM framework. LangSmith seamlessly integrates with LangChain's open-source framework called LangChain, which is widely used for building applications with LLMs.\\n\\nLangSmith provides full visibility into model inputs and outputs at every step in the chain of events, making it easier to debug and analyze the behavior of LLM applications. It has been tested with early design partners and on internal workflows, and it has been found to help teams in various ways.\\n\\nYou can find more information about LangSmith on the official LangSmith documentation [here](https://docs.smith.langchain.com/). Additionally, you can read about the announcement of LangSmith as a unified platform for debugging and testing LLM applications [here](https://blog.langchain.dev/announcing-langsmith/).\",\n",
    "    ),\n",
    "    (\n",
    "        \"What is the langsmith cookbook?\",\n",
    "        \"I'm sorry, but I couldn't find any information about the \\\"Langsmith Cookbook\\\". It's possible that it may not be a well-known cookbook or it may not exist. Could you provide more context or clarify the name?\",\n",
    "    ),\n",
    "    (\n",
    "        \"What is LangChain?\",\n",
    "        \"I'm sorry, but I couldn't find any information about \\\"LangChain\\\". Could you please provide more context or clarify your question?\",\n",
    "    ),\n",
    "    (\"When was Llama-v2 released?\", \"Llama-v2 was released on July 18, 2023.\"),\n",
    "]\n",
    "\n",
    "# Loop through the example data to create runs in your project.\n",
    "for input_, output_ in example_data:\n",
    "    client.create_run(\n",
    "        name=\"ExampleRun\", # The name of the run.\n",
    "        run_type=\"chain\", # The type of the run.\n",
    "        inputs={\"input\": input_}, # The inputs to the run.\n",
    "        outputs={\"output\": output_}, # The outputs of the run.\n",
    "        project_name=project_name, # The project to associate the run with.\n",
    "        end_time=datetime.utcnow(), # The end time of the run.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bf771-6da1-4a98-840f-8800f07a8c9e",
   "metadata": {},
   "source": [
    "## Step 1: Select Runs to Evaluate\n",
    "\n",
    "The first step in our feedback pipeline is to select the runs we want to score. The LangSmith client's `list_runs` method provides a powerful way to filter runs. You can filter by project, time, presence of errors, metadata tags, and more. \n",
    "\n",
    "In this example, we'll filter for all successful runs in our project that occurred since midnight UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9216ab8-4a0e-438b-80c5-50a2427df910",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the current time and set it to midnight UTC.\n",
    "midnight = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# Fetch the list of runs from the specified project.\n",
    "runs = list(\n",
    "    client.list_runs(\n",
    "        project_name=project_name, # Filter by the project name.\n",
    "        execution_order=1, # Fetch in chronological order.\n",
    "        start_time=midnight, # Filter for runs that started after midnight.\n",
    "        error=False # Filter for runs that completed successfully.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec884ee8-c4af-4bc5-b9e7-e87fa4c4f97f",
   "metadata": {},
   "source": [
    "With our target runs selected, it's time to define the feedback logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efb84a-c84a-4903-95a8-e6c0d1e47fa4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Define Feedback Logic\n",
    "\n",
    "Now we'll define the algorithms that generate our feedback scores. Any function or chain can be used. We will demonstrate three different approaches.\n",
    "\n",
    "#### Example A: Simple Text Statistics\n",
    "\n",
    "First, we'll show how to apply a simple, non-LLM algorithm. We will use the `textstat` library to compute various readability scores (like Flesch reading ease) for the *input* to each run. This can be useful for understanding the complexity of user queries your application is receiving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fadbbe-d641-4a37-abd8-e1a57dd1f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\\n"
     ]
    }
   ],
   "source": [
    "# # Install the textstat library.\n",
    "# %pip install textstat --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b2d53f-6a4c-4e08-bc8e-93ebe28574d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textstat # Import the textstat library.\n",
    "from langsmith.schemas import Run, Example # Import the Run and Example schemas.\n",
    "from langchain_core.runnables import RunnableLambda # Import RunnableLambda for batch processing.\n",
    "\n",
    "\n",
    "# Define a function to compute and log text statistics for a single run.\n",
    "def compute_stats(run: Run) -> None:\n",
    "    # Check if the run has the 'input' key we want to measure.\n",
    "    if \"input\" not in run.inputs:\n",
    "        return\n",
    "    # Check if this run has already been scored to avoid redundant work.\n",
    "    if run.feedback_stats and \"smog_index\" in run.feedback_stats:\n",
    "        return\n",
    "    text = run.inputs[\"input\"] # Get the input text.\n",
    "    try:\n",
    "        # A list of readability metric functions to compute.\n",
    "        fns = [\n",
    "            \"flesch_reading_ease\",\n",
    "            \"flesch_kincaid_grade\",\n",
    "            \"smog_index\",\n",
    "            \"coleman_liau_index\",\n",
    "            \"automated_readability_index\",\n",
    "        ]\n",
    "        # Compute each metric and store it in a dictionary.\n",
    "        metrics = {fn: getattr(textstat, fn)(text) for fn in fns}\n",
    "        # Loop through the computed metrics.\n",
    "        for key, value in metrics.items():\n",
    "            # Use the client to create feedback for the original run.\n",
    "            client.create_feedback(\n",
    "                run.id, # The ID of the run to attach feedback to.\n",
    "                key=key, # The name of the metric (e.g., 'smog_index').\n",
    "                score=value,  # The numeric score, used for monitoring charts.\n",
    "                feedback_source_type=\"model\", # Specify the source as 'model' or 'auto'.\n",
    "            )\n",
    "    except Exception:\n",
    "        # Pass silently if textstat fails on a given input.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eadd5bf8-f91a-410c-b246-72b75d8d290e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Wrap our function in a RunnableLambda and use .batch() to apply it concurrently to all runs.\n",
    "_ = RunnableLambda(compute_stats).batch(\n",
    "    runs,\n",
    "    {\"max_concurrency\": 10}, # Control the level of concurrency.\n",
    "    return_exceptions=True, # Prevent the whole batch from failing if one run errors.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b85c62-df99-4b21-a2c3-61137a3c0627",
   "metadata": {},
   "source": [
    "#### Example B: AI-Assisted Feedback\n",
    "\n",
    "While simple statistics are useful, **AI-assisted feedback** is much more powerful. Here, we'll use an LLM as a judge to score our runs on more subjective or complex criteria. This allows you to create metrics that are highly specific to your application's goals.\n",
    "\n",
    "In this example, we will create an evaluator chain that scores each user query along several axes: `relevance` (to LangChain), `difficulty`, `verbosity`, and `specificity`. We will use a pre-built prompt from the LangChain Hub and OpenAI's function-calling feature to ensure the LLM returns a structured JSON output with these scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db72c58-ae92-47bc-9f0b-6e1688c91c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub # Import the LangChain Hub client.\n",
    "\n",
    "# Pull a pre-made prompt for this task from the Hub.\n",
    "prompt = hub.pull(\n",
    "    \"wfh/automated-feedback-example\", api_url=\"https://api.hub.langchain.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d22fcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['prediction', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'wfh', 'lc_hub_repo': 'automated-feedback-example', 'lc_hub_commit_hash': '36dd15a97ff473f5629e56cb327f5d20a4c3b01c4fda0bfce29d678843b46a08'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"You are grading user questions posed to LangChain's technical discussion board. LangChain is a software framework for building applications with large language models.\\n You must rate the questions by relevance, difficulty, verbosity, and specificity.\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['prediction', 'question'], input_types={}, partial_variables={}, template='Grade the following question on a scale from 0 to 5:\\n\\n<question>\\n{question}\\n</question>\\n\\nThe bot responded:\\n\\n<response>\\n{prediction}\\n</response>'), additional_kwargs={}), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Respond in JSON format, giving each value a score between 0 and 5.'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccbdd724-5b1d-410c-bc0d-124800d197df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser # Import the function output parser.\n",
    "from langchain_core.tracers.context import collect_runs # Import a context manager to capture traces.\n",
    "from langchain_openai import ChatOpenAI # Import the OpenAI chat model wrapper.\n",
    "\n",
    "# Define the evaluator chain.\n",
    "chain = (\n",
    "    prompt\n",
    "    # Bind a function-calling schema to the LLM to force structured output.\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=1).bind(\n",
    "        functions=[\n",
    "            {\n",
    "                \"name\": \"submit_scores\",\n",
    "                \"description\": \"Submit the graded scores for a user question and bot response.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"relevance\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5, \"description\": \"Score indicating the relevance of the question to LangChain/LangSmith.\"},\n",
    "                        \"difficulty\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5, \"description\": \"Score indicating the complexity or difficulty of the question.\"},\n",
    "                        \"verbosity\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5, \"description\": \"Score indicating how verbose the question is.\"},\n",
    "                        \"specificity\": {\"type\": \"integer\", \"minimum\": 0, \"maximum\": 5, \"description\": \"Score indicating how specific the question is.\"},\n",
    "                    },\n",
    "                    \"required\": [\"relevance\", \"difficulty\", \"verbosity\", \"specificity\"],\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    | JsonOutputFunctionsParser() # Parse the LLM's function call into a JSON object.\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to evaluate a single run.\n",
    "def evaluate_run(run: Run) -> None:\n",
    "    try:\n",
    "        if \"input\" not in run.inputs or not run.outputs or \"output\" not in run.outputs:\n",
    "            return\n",
    "        if run.feedback_stats and \"specificity\" in run.feedback_stats:\n",
    "            return\n",
    "        # Use collect_runs to capture the trace of the evaluator chain itself.\n",
    "        with collect_runs() as cb:\n",
    "            result = chain.invoke(\n",
    "                {\n",
    "                    \"question\": run.inputs[\"input\"][:3000],  # Truncate to avoid context length issues.\n",
    "                    \"prediction\": run.outputs[\"output\"][:3000],\n",
    "                },\n",
    "            )\n",
    "            # Loop through the scores returned by the evaluator chain.\n",
    "            for feedback_key, value in result.items():\n",
    "                score = int(value) / 5 # Normalize the score to be between 0 and 1.\n",
    "                # Create the feedback for the original run.\n",
    "                client.create_feedback(\n",
    "                    run.id,\n",
    "                    key=feedback_key,\n",
    "                    score=score,\n",
    "                    # Link the feedback to the evaluator's run trace for auditability.\n",
    "                    source_run_id=cb.traced_runs[0].id,\n",
    "                    feedback_source_type=\"model\",\n",
    "                )\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "\n",
    "wrapped_function = RunnableLambda(evaluate_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e061a9-8d0e-4474-872a-2729785027f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concurrently apply the AI-assisted feedback logic to all runs.\n",
    "_ = wrapped_function.batch(runs, {\"max_concurrency\": 10}, return_exceptions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4-f5a6-b7c8-d9e0-f4a4b5c6d7e8",
   "metadata": {},
   "source": [
    "After the feedback has been logged, you can read the project's aggregate feedback stats. It may take a few moments for the stats to update asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "088e262f-6f82-4e0b-afee-e414cc9bd38b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The project's feedback_stats are updated asynchronously.\n",
    "client.read_project(project_name=project_name).feedback_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59c106-7a98-41ef-a769-78005b6ab6c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Example C: Using LangChain Evaluators\n",
    "\n",
    "LangChain provides a number of pre-built, reference-free evaluators that you can use out-of-the-box. These can be easily integrated into a feedback pipeline. For more details on the available types, check out the [LangChain evaluation documentation](https://python.langchain.com/docs/guides/productionization/evaluation).\n",
    "\n",
    "Below, we will demonstrate this by wrapping a `criteria` evaluator in a custom `RunEvaluator`. The criterion we'll use is \"completeness\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb7a8e56-ad86-4e7e-809b-f86704f0cd2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional # Import typing hints.\n",
    "from langchain import evaluation, callbacks # Import LangChain evaluation components.\n",
    "from langsmith import evaluation as ls_evaluation # Import LangSmith evaluation components.\n",
    "\n",
    "\n",
    "# Define our custom evaluator class, inheriting from the base RunEvaluator.\n",
    "class CompletenessEvaluator(ls_evaluation.RunEvaluator):\n",
    "    def __init__(self):\n",
    "        # Define the criterion for the evaluator.\n",
    "        criteria_description = (\n",
    "            \"Does the answer provide sufficient and complete information\"\n",
    "            \"to fully address all aspects of the question (Y)?\"\n",
    "            \" Or does it lack important details (N)?\"\n",
    "        )\n",
    "        # Load the built-in 'criteria' evaluator with our custom criterion.\n",
    "        self.evaluator = evaluation.load_evaluator(\n",
    "            \"criteria\", criteria={\"completeness\": criteria_description}\n",
    "        )\n",
    "\n",
    "    # This is the core method that will be called for each run.\n",
    "    def evaluate_run(\n",
    "        self, run: Run, example: Optional[Example] = None\n",
    "    ) -> ls_evaluation.EvaluationResult:\n",
    "        # Safety check for required fields.\n",
    "        if (\n",
    "            not run.inputs\n",
    "            or not run.inputs.get(\"input\")\n",
    "            or not run.outputs\n",
    "            or not run.outputs.get(\"output\")\n",
    "        ):\n",
    "            return ls_evaluation.EvaluationResult(key=\"completeness\", score=None)\n",
    "        question = run.inputs[\"input\"]\n",
    "        prediction = run.outputs[\"output\"]\n",
    "        # Use collect_runs to capture the trace of the evaluator itself.\n",
    "        with callbacks.collect_runs() as cb:\n",
    "            result = self.evaluator.evaluate_strings(\n",
    "                input=question, prediction=prediction\n",
    "            )\n",
    "            run_id = cb.traced_runs[0].id\n",
    "        # Return the result, linking the feedback to the evaluator's trace.\n",
    "        return ls_evaluation.EvaluationResult(\n",
    "            key=\"completeness\", evaluator_info={\"__run\": {\"run_id\": run_id}}, **result\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd061c7-039c-4799-8eeb-2acffc082545",
   "metadata": {},
   "source": [
    "By using `collect_runs` and passing the resulting run ID to the `evaluator_info` dictionary, we create a direct link in the LangSmith UI from the feedback score on the original run to the trace of the evaluator that produced that score. This is extremely useful for auditing and debugging your feedback logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fe87cd7-af8d-4e72-a994-fbf6c35a8672",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator = CompletenessEvaluator() # Instantiate our completeness evaluator.\n",
    "\n",
    "# You could run this in a simple for loop:\n",
    "# for run in runs:\n",
    "#     client.evaluate_run(run, evaluator)\n",
    "\n",
    "# Or, run it concurrently for better performance.\n",
    "# The `client.evaluate_run` method handles both scoring and logging the feedback.\n",
    "wrapped_function = RunnableLambda(lambda run: client.evaluate_run(run, evaluator))\n",
    "_ = wrapped_function.batch(runs, {\"max_concurrency\": 10}, return_exceptions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e66e144-141a-4d23-b7c3-3627d8ae6265",
   "metadata": {},
   "source": [
    "Check out your project in LangSmith again to see the new \"completeness\" feedback scores appear on your runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb41ef-3892-4385-8830-c6decfbf8f5c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully set up an algorithmic feedback pipeline to programmatically add quality scores to your traced runs. This is a powerful technique for enhancing your monitoring capabilities, helping you curate high-quality datasets for fine-tuning, and gaining deeper insights into your application's usage in production."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents-eval-techniques (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
