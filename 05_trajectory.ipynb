{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734ab45a-eac6-46bb-bed7-2eaac6ab701a",
   "metadata": {},
   "source": [
    "# Evaluating Agents' Intermediate Steps\n",
    "\n",
    "====================================\n",
    "\n",
    "### 1. Theory: Beyond the Final Answer - Evaluating an Agent's Process\n",
    "\n",
    "An **LLM Agent** is a system that uses a Large Language Model as its reasoning engine to make decisions and take actions. It's not just a simple input-to-output model; it can use a suite of tools (like search engines, calculators, or APIs) to gather information and solve complex problems. \n",
    "\n",
    "Because of this, evaluating an agent solely on its final answer is often insufficient. A correct answer might be a lucky guess, or it might have been reached through an inefficient or costly sequence of actions. To build robust and reliable agents, we must also evaluate their **intermediate steps**, or their \"trajectory.\" This means assessing the agent's decision-making process: *Did it choose the right tool for the job? Did it use the tools in a logical order? Was it efficient?*\n",
    "\n",
    "[![Example Agent Trace](./img/agent_trace.png)](https://smith.langchain.com/public/6d50f517-115f-4c14-97b2-2e19b15efca7/r)\n",
    "\n",
    "This notebook provides a walkthrough on how to create a **custom run evaluator** in LangSmith to assess an agent based on its sequence of tool calls. We will compare the agent's actual actions against a predefined, expected sequence to ensure it is behaving as intended.\n",
    "\n",
    "The process involves these key steps:\n",
    "\n",
    "- **Prepare a specialized dataset**: The dataset will contain not just input questions and reference answers, but also the expected sequence of tool calls.\n",
    "- **Define the agent**: We will build an agent with a specific set of tools.\n",
    "- **Construct a custom evaluator**: We will write a Python function to compare the agent's actual tool trajectory with the expected one.\n",
    "- **Run the evaluation**: We will use LangSmith to run the agent against the dataset and apply our custom evaluator.\n",
    "\n",
    "By the end of this guide, you'll have a clear understanding of how to evaluate the internal workings of your agents, leading to more predictable and reliable applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-4a7b-8c9d-1e2f3a4b5c6d",
   "metadata": {},
   "source": [
    "### 2. Prerequisites and Setup\n",
    "\n",
    "First, we'll install the necessary Python packages for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fdc4da-d656-47eb-9298-198a5a7a3834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The '%pip install' command installs python packages from the notebook.\n",
    "# -U flag ensures we get the latest versions of langchain and openai.\n",
    "# %pip install -U langchain openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6a7-b8c9-d0e1-f2a3b4c5d6e7",
   "metadata": {},
   "source": [
    "Next, we configure our environment variables. This is a secure way to provide API keys to our application.\n",
    "\n",
    "- **`LANGCHAIN_API_KEY`**: Your secret key for authenticating with LangSmith.\n",
    "- **`OPENAI_API_KEY`**: Your secret key for the OpenAI API, required for the agent's LLM.\n",
    "- **`LANGCHAIN_ENDPOINT`**: This URL directs all LangChain tracing data to the LangSmith platform.\n",
    "\n",
    "**Action Required**: You must replace the placeholder values with your actual keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cd1303f-a7bb-4098-bf13-598d9893d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv # Import function to load environment variables\n",
    "import os # Import the 'os' module to interact with the operating system.\n",
    "\n",
    "# Load environment variables from the .env file. The `override=True` argument\n",
    "# ensures that variables from the .env file will overwrite existing environment variables.\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "\n",
    "\n",
    "\n",
    "# Update with your API URL if using a hosted instance of Langsmith.\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\" # Set the LangSmith API endpoint as an environment variable.\n",
    "# Update with your API key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGSMITH_API_KEY')# Set your LangSmith API key as an environment variable.\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY') # Set your OpenAI API key as an environment variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526e117",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the Dataset\n",
    "\n",
    "To evaluate an agent's trajectory, our dataset needs a special structure. In addition to the `input` (the question) and a `reference` (the ground-truth final answer), we will include a new field: **`expected_steps`**. This field will contain a list of tool names in the precise order we expect the agent to call them.\n",
    "\n",
    "This `expected_steps` list will serve as the ground truth for our custom trajectory evaluator. Note that for simple queries that don't require a tool, we can provide an empty list `[]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129154f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['7c0955df-98da-4517-ad77-713f0638c238',\n",
       "  '21a5b2e1-ce2f-4f3f-a9ca-ce995ba134a0',\n",
       "  'bc82f54c-58ac-49a2-96f2-7a6adee245ba',\n",
       "  'f835fcfe-0547-43dd-b48b-8f383ec3ff5b',\n",
       "  'b44ec761-91a7-412b-be2c-622d3377f76a'],\n",
       " 'count': 5}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid # Import the uuid library to generate unique identifiers.\n",
    "\n",
    "from langsmith import Client # Import the Client class to interact with LangSmith.\n",
    "\n",
    "client = Client() # Instantiate the LangSmith client.\n",
    "\n",
    "# Define the list of questions and their corresponding outputs.\n",
    "questions = [\n",
    "    (\n",
    "        \"Why was was a $10 calculator app one of the best-rated Nintendo Switch games?\",\n",
    "        {\n",
    "            \"reference\": \"It became an internet meme due to its high price point.\", # The ground-truth final answer.\n",
    "            \"expected_steps\": [\"duck_duck_go\"], # The expected sequence of tool calls.\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"hi\",\n",
    "        {\n",
    "            \"reference\": \"Hello, how can I assist you?\", # The expected direct response.\n",
    "            \"expected_steps\": [],  # Expect a direct response with no tools used.\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Who is Dejan Trajkov?\",\n",
    "        {\n",
    "            \"reference\": \"Macedonian Professor, Immunologist and Physician\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Who won the 2023 U23 world wresting champs (men's freestyle 92 kg)\",\n",
    "        {\n",
    "            \"reference\": \"Muhammed Gimri from turkey\",\n",
    "            \"expected_steps\": [\"duck_duck_go\"],\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What's my first meeting on Friday?\",\n",
    "        {\n",
    "            \"reference\": 'Your first meeting is 8:30 AM for \"Team Standup\"',\n",
    "            \"expected_steps\": [\"check_calendar\"],  # Only expect the calendar tool to be used.\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "uid = uuid.uuid4() # Generate a new unique identifier.\n",
    "dataset_name = f\"Agent Eval Example {uid}\" # Create a unique name for the dataset.\n",
    "# Create the dataset on the LangSmith platform.\n",
    "ds = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"An example agent evals dataset using search and calendar checks.\",\n",
    ")\n",
    "# Create the examples in the dataset.\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q[0]} for q in questions], # The inputs are a list of question dictionaries.\n",
    "    outputs=[q[1] for q in questions], # The outputs are a list of the corresponding output dictionaries.\n",
    "    dataset_id=ds.id, # Link these examples to the dataset we just created.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc0a4c-e9a9-490c-91af-c702e35fad08",
   "metadata": {},
   "source": [
    "## Step 2: Define the Agent\n",
    "\n",
    "An agent is composed of several key parts:\n",
    "- **LLM**: The core reasoning engine that decides which action to take.\n",
    "- **Tools**: The functions the agent can call to interact with the outside world (e.g., search, database lookups).\n",
    "- **Prompt**: A template that instructs the LLM on its role, provides it with the available tools, and formats the conversation history.\n",
    "- **Agent Executor**: The runtime that orchestrates the agent's loop. It calls the agent, executes the chosen tool, passes the result back to the agent, and repeats until a final answer is produced.\n",
    "\n",
    "In this example, we will create an agent that has access to two tools:\n",
    "1. A `DuckDuckGoSearchResults` tool for general web searches.\n",
    "2. A custom `check_calendar` tool, which we create using the `@tool` decorator, to simulate checking a user's calendar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baa875e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse # A utility to parse date strings into datetime objects.\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent # Import core agent components.\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions # A formatting helper.\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser # The output parser.\n",
    "from langchain_openai import ChatOpenAI # The OpenAI chat model wrapper.\n",
    "from langchain_community.tools import DuckDuckGoSearchResults # The DuckDuckGo search tool.\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder # Prompting utilities.\n",
    "from langchain_core.tools import tool # The decorator for creating custom tools.\n",
    "from langchain_core.utils.function_calling import format_tool_to_openai_function # A function formatting helper.\n",
    "\n",
    "\n",
    "# The '@tool' decorator easily turns a Python function into a LangChain tool.\n",
    "@tool\n",
    "def check_calendar(date: str) -> list:\n",
    "    \"\"\"Check the user's calendar for a meetings on the specified datetime (in iso format).\"\"\" # The docstring is used as the tool's description for the agent.\n",
    "    date_time = parse(date) # Parse the input date string.\n",
    "    # This is a mock implementation to demonstrate the concept.\n",
    "    if date_time.weekday() == 4: # 4 corresponds to Friday.\n",
    "        return [\n",
    "            \"8:30 : Team Standup\",\n",
    "            \"9:00 : 1 on 1\",\n",
    "            \"9:45 design review\",\n",
    "        ]\n",
    "    return [\"Focus time\"] # Return a default for other days.\n",
    "\n",
    "\n",
    "# Define the main function that creates and runs our agent.\n",
    "def agent(inputs: dict):\n",
    "    # Initialize the LLM. We use a model that's good at function calling.\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        temperature=0, # Set temperature to 0 for more deterministic, repeatable outputs.\n",
    "    )\n",
    "    # Define the list of tools the agent has access to.\n",
    "    tools = [\n",
    "        DuckDuckGoSearchResults(\n",
    "            name=\"duck_duck_go\" # Give the tool a specific name.\n",
    "        ),\n",
    "        check_calendar, # Our custom calendar tool.\n",
    "    ]\n",
    "    # Define the prompt template for the agent.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"You are a helpful assistant.\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"), # Placeholder for intermediate steps.\n",
    "            (\"user\", \"{question}\"), # Placeholder for the user's input question.\n",
    "        ]\n",
    "    )\n",
    "    # Create the runnable agent component.\n",
    "    runnable_agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "\n",
    "    # Create the Agent Executor, which orchestrates the agent's runs.\n",
    "    executor = AgentExecutor(\n",
    "        agent=runnable_agent,\n",
    "        tools=tools,\n",
    "        handle_parsing_errors=True, # Gracefully handle any parsing errors.\n",
    "        return_intermediate_steps=True, # CRITICAL: This must be True to get the trajectory for evaluation.\n",
    "    )\n",
    "    # Invoke the executor with the inputs.\n",
    "    return executor.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464d318",
   "metadata": {},
   "source": [
    "## Step 3: Define the Custom Evaluator\n",
    "\n",
    "Now we'll create our custom evaluator. This is a simple Python function that will receive the agent's run trace (`Run` object) and the corresponding dataset entry (`Example` object).\n",
    "\n",
    "The logic of our `intermediate_step_correctness` function is straightforward:\n",
    "1.  Extract the list of intermediate steps from the agent's final output. Because we set `return_intermediate_steps=True`, the output will contain an `\"intermediate_steps\"` key.\n",
    "2.  Process this list to get only the names of the tools that were called, creating the agent's actual trajectory.\n",
    "3.  Retrieve the `\"expected_steps\"` list from the example's outputs.\n",
    "4.  Compare the actual trajectory to the expected trajectory.\n",
    "5.  Return a dictionary with a unique `key` for the evaluation metric and a `score` (1 for a perfect match, 0 otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7c701d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional # Import typing hints.\n",
    "\n",
    "from langsmith.schemas import Example, Run # Import the Run and Example schemas from LangSmith.\n",
    "\n",
    "\n",
    "# Define the custom evaluator function.\n",
    "def intermediate_step_correctness(run: Run, example: Optional[Example] = None) -> dict:\n",
    "    if run.outputs is None: # A safety check to ensure the run has completed and has outputs.\n",
    "        raise ValueError(\"Run outputs cannot be None\")\n",
    "    # Get the 'intermediate_steps' from the agent's output, defaulting to an empty list if not found.\n",
    "    intermediate_steps = run.outputs.get(\"intermediate_steps\") or []\n",
    "    # The intermediate_steps list contains tuples of (AgentAction, observation).\n",
    "    # We only care about the action's 'tool' attribute.\n",
    "    # This list comprehension extracts the tool name for each step.\n",
    "    trajectory = [action.tool for action, _ in intermediate_steps]\n",
    "    # Retrieve the ground-truth trajectory from our dataset example.\n",
    "    expected_trajectory = example.outputs[\"expected_steps\"]\n",
    "    # Perform a simple equality check between the actual and expected trajectories.\n",
    "    score = int(trajectory == expected_trajectory)\n",
    "    # Return the result in the format required by LangSmith.\n",
    "    return {\"key\": \"Intermediate steps correctness\", \"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8568f90",
   "metadata": {},
   "source": [
    "## Step 4: Run the Evaluation\n",
    "\n",
    "Now we can put everything together and run the evaluation. We will use two evaluators:\n",
    "1.  Our custom `intermediate_step_correctness` function to check the tool trajectory.\n",
    "2.  A standard `\"qa\"` evaluator to check if the agent's final answer is correct based on the `reference` label.\n",
    "\n",
    "Because our dataset's `outputs` field is a dictionary with multiple keys (`reference` and `expected_steps`), the standard `qa` evaluator doesn't know which key to use as the ground-truth answer. We must provide a `prepare_data` helper function to explicitly map the `prediction` to the agent's final `output` and the `reference` to the `reference` key in our example's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5201d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Agent Eval Example-2a07d2eb' at:\n",
      "https://smith.langchain.com/o/0212d326-bd9d-42bb-9937-c063f40f2361/datasets/e79d4c23-37d0-43b9-abd2-b913e3bbbc94/compare?selectedSessions=7aefc064-da35-4b32-a967-4e583e5df9bc\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/tankwin08/Desktop/projects/professional/ai-agents-eval-techniques/.venv/lib/python3.11/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n",
      "/Users/tankwin08/Desktop/projects/professional/ai-agents-eval-techniques/.venv/lib/python3.11/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n",
      "3it [00:07,  2.15s/it]/Users/tankwin08/Desktop/projects/professional/ai-agents-eval-techniques/.venv/lib/python3.11/site-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
      "  with DDGS() as ddgs:\n",
      "5it [00:10,  2.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate # Import the evaluation functions.\n",
    "\n",
    "\n",
    "# Define a data preparation function for the standard QA evaluator.\n",
    "def prepare_data(run: Run, example: Example) -> dict:\n",
    "    # This function creates the specific dictionary that the 'qa' evaluator expects.\n",
    "    return {\n",
    "        \"input\": example.inputs[\"question\"], # The original question.\n",
    "        \"prediction\": run.outputs[\"output\"], # The agent's final answer.\n",
    "        \"reference\": example.outputs[\"reference\"], # The ground-truth final answer.\n",
    "    }\n",
    "\n",
    "\n",
    "# Create an instance of the standard QA evaluator, passing our data preparation function.\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", prepare_data=prepare_data)\n",
    "\n",
    "# Run the full evaluation.\n",
    "chain_results = evaluate(\n",
    "    agent, # The agent function to be tested.\n",
    "    data=dataset_name, # The name of our dataset in LangSmith.\n",
    "    # A list containing both our custom evaluator and the standard QA evaluator.\n",
    "    evaluators=[intermediate_step_correctness, qa_evaluator],\n",
    "    experiment_prefix=\"Agent Eval Example\", # A prefix for the experiment name in LangSmith.\n",
    "    max_concurrency=1, # Run sequentially as some agents/tools may not be thread-safe.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe01d96-bfde-4979-a1e3-c6c70759d9b7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have successfully configured and run a comprehensive evaluation of an LLM agent. You've not only checked its final answers for correctness but have also validated its internal decision-making process by comparing its tool usage against an expected trajectory.\n",
    "\n",
    "This technique of evaluating intermediate steps is crucial for building agents that are not only accurate but also efficient, predictable, and aligned with your intended logic.\n",
    "\n",
    "The simple, direct comparison evaluator we built is a great starting point. For more complex scenarios where multiple tool sequences could be valid, you can extend this concept by using an LLM-powered evaluator to grade the agent's trajectory. LangChain's [TrajectoryEvalChain](https://python.langchain.com/docs/guides/productionization/evaluation/trajectory/trajectory_eval#evaluate-trajectory) provides an excellent off-the-shelf solution for this more advanced use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-agents-eval-techniques (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
